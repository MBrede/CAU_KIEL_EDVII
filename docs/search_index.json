[["index.html", "EDV2 Vorwort", " EDV2 Max Brede 2021-05-05 Vorwort Dieses mit bookdown erstellte Dokument ist das über das Sommersemester 2021 hinweg wachsende Skript zur Übung “PSY_B_12-2: Computerunterstützte Datenanalyse II” der CAU zu Kiel. Dieses Skript steht, wo nicht anders angegeben, unter der CC BY-SA 4.0-Lizenz. "],["lehrplan.html", "Lehrplan Semesterplan Übungsformat Lehrziele für jede Sitzung Prüfungsleistung", " Lehrplan Semesterplan Einheit Vorlesung Übungswoche Thema 1 23.04.21 keine Übung Deskriptive Statistik Data Cleaning 2 07.05.21 KW 19 Hilfsmittel für die Inferenzstatistik Lineare Regression I 3 21.05.21 KW 21 Lineare Regression II 4 04.06.21 KW 23 t- Tests einfaktorielle Varianzanalyse 5 18.06.21 KW 25 zweifaktorielle Varianzanalyse 6 02.07.21 KW 27 Kontrasttests Übungsformat Die Übung soll zur Hälfte in 45-minütigen Sitzungen im Vorlesungsformat zur Vorstellung der Funktionen und zur anderen Hälfte als 45-minütige praktische Übung stattfinden. Es wird pro Übungs-Sitzung ein Übungszettel ausgegeben, der mit Hilfe der in der Zugehörigen Vorlesung besprochenen Funktionen bearbeitet werden können soll. Diese Zettel sollen nach der jeweiligen Vorlesung für die Übungen vorbereitet werden, in denen der Zettel dann besprochen und mögliche Fragen geklärt werden. Nach den Übungssitzungen haben die Studierenden dann eine Woche Zeit, zusätzliche Hausaufgaben zu bearbeiten. Eine Ausnahme von diesem Ablauf ist die erste Sitzung, in der organisatorisches und Grundlagen in 90 minütigem Vorlesungsstil besprochen werden sollen. Auch nach dieser Sitzung werden aber Übungszettel und Hausaufgaben ausgegeben. Lehrziele für jede Sitzung Die Studierenden können nach dem Absolvieren der Übung… Einheit 1 Gründe für deskriptive Statistik nennen. für verschiedene Ausgangssituationen entscheiden, welche Darstellungsform angemessen ist. Verfahren zum Umgang mit fehlenden Werten nennen und anwenden. Verfahren zum Umgang mit Ausreißern nennen und anwenden. Einheit 2 R formulas lesen und verwenden. Korrelationsanalysen in R durchführen und die Ergebnisse interpretieren. einfache lineare Regressionen in R durchführen und die Ergebnisse interpretieren. Einheit 3 multiple lineare Regressionen in R durchführen und die Ergebnisse interpretieren. Einheit 4 t-Tests in R durchführen und die Ergebnisse interpretieren. einfaktorielle Varianzanalysen in R durchführen und die Ergebnisse interpretieren. Einheit 5 zweifaktorielle Varianzanalysen in R durchführen und die Ergebnisse interpretieren. Einheit 6 beliebige Linearkontraste in R durchführen und die Ergebnisse interpretieren. paarweise post-hoc t-Tests in R durchführen und die Ergebnisse interpretieren. Prüfungsleistung Die Studierenden müssen während des Semesters die nach den Übungssitzungen ausgegebenen Hausaufgaben innerhalb einer Woche sinnvoll bearbeitet abgeben. Mit maximal einer nicht sinnvoll bearbeiteten Serie werden die Studierenden zur Gruppenarbeit am Ende des Semesters zugelassen. "],["deskriptive-statistik-und-data-cleaning.html", "Deskriptive Statistik und Data Cleaning Organisatorisches Deskriptive Statistik Data Cleaning Organisationsformen von Datensätzen", " Deskriptive Statistik und Data Cleaning Organisatorisches Semesterplan Einheit Vorlesung Übungswoche Thema 1 23.04.21 keine Übung Deskriptive Statistik Data Cleaning 2 07.05.21 KW 19 Hilfsmittel für die Inferenzstatistik Lineare Regression I 3 21.05.21 KW 21 Lineare Regression II 4 04.06.21 KW 23 t- Tests einfaktorielle Varianzanalyse 5 18.06.21 KW 25 zweifaktorielle Varianzanalyse 6 02.07.21 KW 27 Kontrasttests Übungsablauf Es wird alle zwei Wochen eine 45-minütige Vorlesung geben und dazu alternierend online-Übungsstunden in mit je einem Kurs. (Eine Ausnahme ist die erste Woche, in der wir eine Vorlesung haben.) Übungsablauf Prüfungsleistung Gruppenarbeit(4-5 Personen) über 3 Wochen Termin für die Klausur entweder vor oder mit Anfang in der Klausurphase. Genauere Informationen gibt es über das Olat. Als Zulassung für die Gruppenarbeit ist wieder die sinnvolle Bearbeitung von allen bis auf eine Hausaufgabenserien nötig. Tutorien Ronja und Katharina geben (online-)Tutorien dieses Semester. Katharinas Tutorium wird immer dienstags, 14-16 Uhr stattfinden. Ronjas Tutorium wird immer dienstags, 16-18 Uhr stattfinden. Deskriptive Statistik Wozu brauche ich das? Im Gegensatz zur Inferenzstatistik ist das erklärte Ziel der deskriptiven Statistik, (wie der Name schon sagt) beschreibende Aussagen über die vorliegende Stichprobe zu treffen. Wir wollen uns also möglichst genau angucken, wie unsere Stichprobe aussieht. Wozu könnte das gut sein? Gründe für deskriptive Statistik Indikatoren zur externen Validität(Verteilung von Organismusvariablen, Demografie,…) Aussagen über Verteilungseigenschaften Schnell zu erfassende Präsentationen von zentraler Tendenz und Streuungen Hinweise auf ungewöhnliche Werte (Ausreißer, fehlende Werte,…) Übersicht über Effekte und Zusammenhänge, inklusive solcher, die möglicherweise nicht a-priori erwartet wurden deskriptive Statistik Für einen ganz einfachen, schnellen und umfassenden Überblick über die Daten funktioniert die skim-Funktion aus dem skimr-Paket gut: skimr::skim(df) Tab. 1: Data summary Name df Number of rows 12498 Number of columns 5 _______________________ Column type frequency: character 2 logical 1 numeric 2 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace group 0 1 3 3 0 7 0 sex 0 1 1 1 0 2 0 Variable type: logical skim_variable n_missing complete_rate mean count smoker 0 1 0.5 TRU: 6297, FAL: 6201 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist IQ 0 1 100 15 69.71 87.34 100.57 112.61 126.67 ▅▆▇▇▆ age 0 1 40 20 17.85 25.00 32.16 53.63 88.72 ▇▅▁▂▂ Demografie Einfache, schnell zu erfassende Beschreibung der Stichprobe zum Beispiel über eine Tabelle: df %&gt;% count(sex, smoker, group) %&gt;% pivot_wider(names_from = group, values_from = n) ## # A tibble: 4 x 9 ## sex smoker g_1 g_2 g_3 g_4 g_6 g_7 ## &lt;chr&gt; &lt;lgl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 f FALSE 373 472 609 384 430 482 ## 2 f TRUE 375 447 634 413 451 474 ## 3 m FALSE 361 441 579 394 442 464 ## 4 m TRUE 397 469 617 358 424 437 ## g_8 ## &lt;int&gt; ## 1 390 ## 2 395 ## 3 380 ## 4 406 Aber vielleicht ein bisschen übersichtlicher in einer Grafik: df %&gt;% count(sex, smoker, group) %&gt;% ggplot(aes(x = group, fill = smoker, y = n)) + geom_col(position = &#39;dodge&#39;) + facet_wrap(~sex) + labs(x = &#39;Group&#39;, y = &#39;Count&#39;, fill = &#39;Smoker&#39;) Man sieht auf einen Blick, dass Geschlecht und Raucher ungefähr gleich aufgeteilt wurden, die Gruppen aber wesentlich unterschiedliche Größen aufweisen! Aussagen über Verteilungseigenschaften In der skim-Ausgabe haben wir ja schon sehen können, dass keine fehlenden Werte vorliegen (n_missing war 0). Wir könnten uns aber noch die Frage stellen, ob Extremwerte in den Gruppen auftauchen, außerdem wollen wir möglichst übersichtlich unsere Verteilungseigenschaften präsentieren. Das hilft uns zum Einen, um einen besseren Überblick über die Daten zu erhalten, die wir auswerten wollen, zum Anderen hilft es bei einem späteren Bereicht der Leserin, unsere Statistik einzuschätzen. Dazu können wir uns entweder eine Tabelle mit den Verteilungsparametern der numerischen Variablen ausgeben lassen: df %&gt;% pivot_longer(where(is.numeric), names_to = &#39;variable&#39;) %&gt;% group_by(variable) %&gt;% summarise(across( value, list( mean = ~ mean(.), sd = ~ sd(.), min = ~ min(.), q1 = ~ quantile(., .25), median = ~ median(.), q3 = ~ quantile(., .75), max = ~ max(.) ), .names = &#39;{.fn}&#39; )) %&gt;% mutate(across(where(is.numeric), ~round(.,2))) ## # A tibble: 2 x 8 ## variable mean sd min q1 median q3 max ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 age 40 20 17.8 25 32.2 53.6 88.7 ## 2 IQ 100 15 69.7 87.3 101. 113. 127. Oder, wieder ein bisschen übersichtlicher, in einem Diagramm. So könnten wir die ganzen Infos gerade zum Beispiel in einem Boxplot mit eingezeichneten Mittelwerten und Streuungsbalken darstellen: df %&gt;% pivot_longer(where(is.numeric), names_to = &#39;variable&#39;) %&gt;% ggplot(aes(x = &#39;&#39;, y = value)) + geom_violin(fill = &#39;lightgrey&#39;, color = NA) + geom_boxplot(width = .25) + stat_summary(fun = mean, fun.min = function(x)mean(x) - sd(x), fun.max = function(x)mean(x) + sd(x), color = &#39;red&#39;) + facet_wrap(~variable, scales = &#39;free&#39;) + labs(x = &#39;&#39;, y = &#39;&#39;) Das Alter ist eindeutig schief verteilt, der IQ dafür mehrgipflig. Nach der von Tukey aufgestellten Regel (Tukey 1977) haben wir auch keine Ausreißer (dazu auch später mehr). Darstellung von Effekten und Zusammenhängen Unterschiede Da wir eine Reihe von Gruppierungsvariablen haben, könnte der erste Impuls sein, sich die Variablen nach diesen Gruppen aufgeteilt darstellen zu lassen, um mögliche Gruppenunterschiede zu verdeutlichen. Auch hier können wir uns Tabellen erstellen: df %&gt;% group_by(smoker, group, sex) %&gt;% summarise(across(where(is.numeric), .fns = list(mean = ~mean(.), sd = ~sd(.), n = ~n()))) %&gt;% mutate(across(where(is.numeric), ~round(.,2))) ## # A tibble: 28 x 9 ## # Groups: smoker, group [14] ## smoker group sex IQ_mean IQ_sd IQ_n age_mean ## &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 FALSE g_1 f 99.0 13.9 373 31.8 ## 2 FALSE g_1 m 98.4 13.9 361 30.6 ## 3 FALSE g_2 f 98.9 19.3 472 52.0 ## 4 FALSE g_2 m 97.4 19.0 441 53.0 ## 5 FALSE g_3 f 102. 14.0 609 42.5 ## 6 FALSE g_3 m 102. 13.7 579 41.7 ## 7 FALSE g_4 f 101. 13.8 384 40.8 ## 8 FALSE g_4 m 99.3 13.1 394 42.0 ## 9 FALSE g_6 f 99.3 14.6 430 34.8 ## 10 FALSE g_6 m 101. 15.4 442 35.9 ## age_sd age_n ## &lt;dbl&gt; &lt;dbl&gt; ## 1 17.3 373 ## 2 15.3 361 ## 3 27.9 472 ## 4 27.7 441 ## 5 18.2 609 ## 6 17.4 579 ## 7 19.3 384 ## 8 20.3 394 ## 9 14.6 430 ## 10 14.9 442 ## # … with 18 more rows Oder Plots erstellen um die möglichen Unterschiede darzustellen: df %&gt;% pivot_longer(cols = where(is.numeric), names_to = &#39;variable&#39;) %&gt;% group_by(variable, smoker, sex, group) %&gt;% summarise(m = mean(value), sem = sqrt(var(value)/n()), upper = m + sem, lower = m - sem) %&gt;% ggplot(aes(x = group, y = m, fill = smoker)) + geom_col(position = &#39;dodge&#39;) + geom_errorbar(aes(ymin = lower, ymax = upper), position = &#39;dodge&#39;)+ facet_grid( variable ~ sex , scales = &#39;free&#39;) Das wird zwar ein bisschen unübersichtlich (wenn man das wirklich sinnvoll betreiben wollen würde sollte man sich Gedanken dazu machen, welche Variablen tatsächlich von Relevanz sind), man könnte aber zu dem Schluss kommen dass die IQs relativ ähnlich sind, die Altersgruppen aber nicht. Zusammenhänge Zuletzt wollen wir noch gucken, ob in den Daten irgendwelche (linearen) Zusammenhänge direkt ersichtlich sind. Dazu können wir zuerst Korrelationen berechnen, zum Beispiel einmal für die gesamte Stichprobe und einmal für die Untergruppen: library(magrittr) df %$% cor(age, IQ) ## [1] 0.06659135 df %&gt;% group_by(group) %&gt;% summarise(r = cor(age, IQ)) ## # A tibble: 7 x 2 ## group r ## &lt;chr&gt; &lt;dbl&gt; ## 1 g_1 0.0540 ## 2 g_2 0.00747 ## 3 g_3 0.110 ## 4 g_4 0.0952 ## 5 g_6 0.111 ## 6 g_7 0.139 ## 7 g_8 0.0636 Hier ist so weit nichts auffällig. Ein letzter zu überprüfender Aspekt sind die nicht-linearen Zusammenhänge, zum Beispiel über angemessene Plots. Dies können wir zum Einen für die Untergruppen überprüfen wollen: df %&gt;% ggplot(aes(x = IQ, y = age, color = group)) + geom_point() + facet_grid(smoker ~ sex) Zum Anderen für die gesamte Stichprobe: df %&gt;% ggplot(aes(IQ, age, color = group)) + geom_point(size = .001) + scale_color_grey() Abb. 1: Original-Comic von xkcd Data Cleaning Umgang mit fehlenden Werten NAs sind das in R zur Codierung von fehlenden Werten genutzte Datenformat. Sie können in Vektoren (und damit auch tibble-Spalten) jeden Datenformats auftreten: c(T,NA,F) ## [1] TRUE NA FALSE c(1,NA,2) ## [1] 1 NA 2 c(&#39;a&#39;,NA,&#39;b&#39;) ## [1] &quot;a&quot; NA &quot;b&quot; Wenn wir versuche mit einem Vektor zu rechnen, der NAs beinhaltet, können wir auf Probleme stoßen: aVector &lt;- c(NA,1,2,5,NA,6,8) length(aVector[aVector &gt; 3]) ## [1] 5 mean(aVector) ## [1] NA Mit der is.na()-Funktion können wir uns einen logischen Vektor ausgeben lassen, der fehlende Werte codiert. Den können wir dann wie gewohnt benutzen: sum(is.na(aVector)) ## [1] 2 any(is.na(aVector)) ## [1] TRUE fehlende Werte und einfache Kennwerte Die meisten Funktionen im base R Umfang haben ein na.rm-Argument mit dem wir fehlende Werte von Berechnungen ausschließen können. Das kann an vielen Stellen schon eine sinnvolle Lösung sein, zum Beispiel wenn wir die Infos über die Anzahl fehlender Werte nicht verlieren wollen. Das könnte dann zum Beispiel so aussehen: mean(aVector) ## [1] NA mean(aVector, na.rm = T) ## [1] 4.4 Dieses Argument können wir auch in die gewohnten Pipelines einsetzen. Als Beispiel nehmen wir diesen kleinen (unrealistisch unvollständigen) Datensatz df_2: (df_2 &lt;- read_csv(&#39;data/small_nas.csv&#39;)) ## # A tibble: 12 x 4 ## VP group t_1 t_2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 3.66 -1.53 ## 2 2 2 NA 3.32 ## 3 3 3 2.23 NA ## 4 4 4 4.82 4.14 ## 5 5 1 0.142 0.537 ## 6 6 2 1.86 5.04 ## 7 7 3 NA 1.46 ## 8 8 4 3.60 3.50 ## 9 9 1 0.353 NA ## 10 10 2 NA NA ## 11 11 3 3.79 2.57 ## 12 12 4 5.37 4.95 Wir könnten jetzt die Pipeline für die Gruppenunterschiede von eben nochmal benutzen, aber um eine Angabe zur Anzahl der fehlenden Werte ergänzen: df_2 %&gt;% group_by(group) %&gt;% summarise(across(matches(&#39;t_&#39;), .fns = list(mean = ~mean(., na.rm = T), sd = ~sd(., na.rm = T), n = ~n(), missing = ~sum(is.na(.))))) %&gt;% mutate(across(where(is.numeric), ~round(.,2))) ## # A tibble: 4 x 9 ## group t_1_mean t_1_sd t_1_n t_1_missing t_2_mean ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.39 1.97 3 0 -0.5 ## 2 2 1.86 NA 3 2 4.18 ## 3 3 3.01 1.1 3 1 2.01 ## 4 4 4.6 0.91 3 0 4.2 ## t_2_sd t_2_n t_2_missing ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.46 3 1 ## 2 1.22 3 1 ## 3 0.79 3 1 ## 4 0.73 3 0 NA-Bereinigung von Datensätzen Vor statistischen Auswertungen kann es aber einfacher sein, den Datensatz komplett von fehlenden Werten zu bereinigen. Je nach dem Fall und der Person die man fragt, gibt es verschiedene Vorgehensweisen. Wir gucken uns hier genauer den fallweisen Ausschluss und das ersetzen durch Werte der zentralen Tendenz an. Die Entscheidung für das Auffüllen oder das Ausschließen muss von Fall zu Fall gefällt werden! Wenn wir zum Beispiel unseren df_2 nochmal angucken, fehlt ein Viertel der Werte. Hier die Fälle aufzufüllen und so zu tun als würde man mit 133% der Werte arbeiten, die tatsächlich vorlagen, ist offensichtlich schwierig. Gleichzeitig wird oft das Argument vorgebracht, dass insbesondere diejenigen Versuchspersonen, die in bestimmten Bedingungen keine Antwort produziert haben ein wichtiger Teil der Stichprobe sind und das Auslassen an sich als Form der Antwort betrachtet werden kann. Wenn wir diese Versuchspersonen ausschließen, verzerren wir nach diesem Argument also systematisch unsere Stichprobe. Wichtig ist also vor jeder Bereinigung, Überlegungen darüber anzustellen, was im gegebenen Fall gerade die angemessenste Lösung darstellt. Bei unserem Datensatz df_2 sind beide Methoden nicht wirklich gut, der Datensatz ist aber auch extrem. Fallweiser Ausschluss Die radikalste Methode ist der Fallweise Ausschluss, also der Ausschluss aller Eintragungen einer Versuchsperson, die mindestens einen fehlenden Wert vorliegen hat. Als Erinnerung, hier nochmal unser Datensatz df_2: VP group t_1 t_2 1 1 3.6612191 -1.5337696 2 2 NA 3.3223479 3 3 2.2250924 NA 4 4 4.8187796 4.1387727 5 1 0.1419190 0.5373972 6 2 1.8635821 5.0435449 7 3 NA 1.4552116 8 4 3.6001703 3.5013055 9 1 0.3530444 NA 10 2 NA NA 11 3 3.7877141 2.5676322 12 4 5.3706695 4.9490249 Wir müssten also die Versuchspersonen ausschließen. Die einfachste Variante dafür ist, den Datensatz ins wide-Format zu überführen (wie er es in unserem Fall schon vorliegt) und mit drop_na diejenigen Zeilen auszuschließen, die fehlende Werte beinhalten: df_2 %&gt;% drop_na() ## # A tibble: 7 x 4 ## VP group t_1 t_2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 3.66 -1.53 ## 2 4 4 4.82 4.14 ## 3 5 1 0.142 0.537 ## 4 6 2 1.86 5.04 ## 5 8 4 3.60 3.50 ## 6 11 3 3.79 2.57 ## 7 12 4 5.37 4.95 Ersetzen fehlender Werte Statt radikal alle Fälle auszuschließen, die mindestens einen fehlenden Wert beinhalten, gibt es auch Ansätze, diese aufzufüllen. Gängige Verfahren hier sind die fehlenden Werte hypothesenunabhängig (also nicht gruppenweise) mit dem (getrimmten) Mittelwert, dem Median oder dem Modus der Gesamtstichprobe aufzufüllen. Die Umsetzung in R sieht dann immer gleich aus, die einzige Änderung findet im Kennwert statt, den man zur Ergänzung wählt. Hier mal ein Beispiel mit dem getrimmten Mittelwert: df_2 %&gt;% mutate(across(where(is.numeric), ~replace_na(., mean(., na.rm=T, trim = .05)))) ## # A tibble: 12 x 4 ## VP group t_1 t_2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 3.66 -1.53 ## 2 2 2 2.87 3.32 ## 3 3 3 2.23 2.66 ## 4 4 4 4.82 4.14 ## 5 5 1 0.142 0.537 ## 6 6 2 1.86 5.04 ## 7 7 3 2.87 1.46 ## 8 8 4 3.60 3.50 ## 9 9 1 0.353 2.66 ## 10 10 2 2.87 2.66 ## 11 11 3 3.79 2.57 ## 12 12 4 5.37 4.95 Umgang mit Ausreißern Ausreißerbereinigung sind ein komplexes Thema, über das viel diskutiert werden kann und auch muss. Da wir uns hier aber im Rahmen einer praktischen Übung befinden sparen wir uns das und nutzen die weit verbreitete Regel, die Tukey (1977) formuliert hat: Diejenigen Werte sind als Ausreißer zu betrachten, die außerhalb des Intervals \\[Q_1 - 1.5 \\cdot \\text{IQR} \\leq x \\leq Q_3 + 1.5 \\cdot \\text{IQR}\\] liegen. Diese Regel ist auch der in geom_boxplot implementierte Standard, den wir ja auch schon zumindest vom Sehen kennen. Die Frage ist nun, was wir mit eventuell detektierten Ausreißern machen. Dafür betrachten wir den folgenden Datensatz df_3: df_3 %&gt;% pivot_longer(cols = everything()) %&gt;% ggplot(aes(y = value, x = &#39;&#39;)) + geom_boxplot() + geom_dotplot(binaxis = &#39;y&#39;, stackdir = &#39;center&#39;, alpha = .5, fill = &#39;red&#39;, dotsize = .5) + facet_wrap(~name,scales = &#39;free_y&#39;) Auch hier können wir wieder zum radikalen Ausschluss greifen und den Datensatz einfach danach filtern, dass unsere Variablen zwischen den “Tukey Fences” liegen: df_3 %&gt;% filter(between(t_1, quantile(t_1,.25) - 1.5 * IQR(t_1), quantile(t_1,.75) + 1.5 * IQR(t_1)) &amp; between(t_2, quantile(t_2,.25) - 1.5 * IQR(t_2), quantile(t_2,.75) + 1.5 * IQR(t_2))) %&gt;% pivot_longer(cols = everything()) %&gt;% ggplot(aes(y = value, x = &#39;&#39;)) + geom_boxplot() + geom_dotplot(binaxis = &#39;y&#39;, stackdir = &#39;center&#39;, alpha = .5, fill = &#39;red&#39;, dotsize = .5) + facet_wrap(~name,scales = &#39;free_y&#39;) Oder wir ‘windsorieren’ unsere Ausreißer, indem wir sie durch den Wert der jeweiligen Grenze ersetzen. Wir sagen also, dass diejenigen Werte, die außerhalb der Fences liegen auf den Wert der jeweiligen Grenze gesetzt werden sollen: df_3 %&gt;% mutate(across(everything(), ~ifelse(. &gt; quantile(.,.75) + 1.5 * IQR(.), quantile(.,.75) + 1.5 * IQR(.), .)), across(everything(), ~ifelse(. &lt; quantile(.,.25) - 1.5 * IQR(.), quantile(.,.25) - 1.5 * IQR(.), .))) %&gt;% pivot_longer(cols = everything()) %&gt;% ggplot(aes(y = value, x = &#39;&#39;)) + geom_boxplot() + geom_dotplot(binaxis = &#39;y&#39;, stackdir = &#39;center&#39;, alpha = .5, fill = &#39;red&#39;, dotsize = .5) + facet_wrap(~name,scales = &#39;free_y&#39;) Organisationsformen von Datensätzen long vs. wide format long-format Name RT Bedingung Snake Müller 2624 1 Snake Müller 3902 2 Snake Müller 6293 3 Vera Baum 1252 1 Vera Baum 2346 2 Vera Baum 4321 3 wide-format Name RT_1 RT_2 RT_3 Snake Müller 2624 3902 6293 Vera Baum 1252 2346 4321 Die pivot-Funktionen pivot_longer und pivot_wider bieten die Möglichkeit, einen Datensatz von einem in das andere Format zu konvertieren. longFormat ## Name RT Bedingung ## 1 Snake Müller 2624 1 ## 2 Snake Müller 3902 2 ## 3 Snake Müller 6293 3 ## 4 Vera Baum 1252 1 ## 5 Vera Baum 2346 2 ## 6 Vera Baum 4321 3 long to wide wideFormat &lt;- longFormat %&gt;% pivot_wider(names_from = &#39;Bedingung&#39;, values_from = &#39;RT&#39;, names_prefix = &#39;RT_&#39;) wideFormat ## # A tibble: 2 x 4 ## Name RT_1 RT_2 RT_3 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Snake Müller 2624 3902 6293 ## 2 Vera Baum 1252 2346 4321 wide to long longFormat &lt;- wideFormat %&gt;% pivot_longer(cols = -1, names_prefix = &#39;RT_&#39;, names_to = &#39;Bedingung&#39;, values_to = &#39;RT&#39;) longFormat ## # A tibble: 6 x 3 ## Name Bedingung RT ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Snake Müller 1 2624 ## 2 Snake Müller 2 3902 ## 3 Snake Müller 3 6293 ## 4 Vera Baum 1 1252 ## 5 Vera Baum 2 2346 ## 6 Vera Baum 3 4321 "],["hilfsmittel-für-die-inferenzstatistik.html", "Hilfsmittel für die Inferenzstatistik Organisatorisches Hilfsmittel für die Inferenzstatistik", " Hilfsmittel für die Inferenzstatistik Organisatorisches Semesterplan Einheit Vorlesung Übungswoche Thema 1 23.04.21 keine Übung Deskriptive Statistik Data Cleaning 2 07.05.21 KW 19 Hilfsmittel für die Inferenzstatistik Lineare Regression I 3 21.05.21 KW 21 Lineare Regression II 4 04.06.21 KW 23 t- Tests einfaktorielle Varianzanalyse 5 18.06.21 KW 25 zweifaktorielle Varianzanalyse 6 02.07.21 KW 27 Kontrasttests Hilfsmittel für die Inferenzstatistik Modellterme Alle inferenzstatistischen Verfahren im base-R-Umfang und viele andere aus Zusatzpaketen nutzen die sogenannte Formelschreibweise um Modelle zu definieren. Am Anfang ist die Syntax ein bisschen ungewohnt, am Ende resultiert aus dieser Schreibweise aber eine sehr übersichtliche und schnell erfassbare Modell-Formulierung. Die Formulierung folgt dabei grundsätzlich dem folgenden System, das sich am Besten analog zu einer mathematischen Funktionsgleichung vorgestellt werden kann. Da das = aber schon für Zuweisungen belegt ist, wird es in formula-Schreibweise durch eine Tilde (~) ersetzt: Tab. 2: modellierte Variable(n)~Modellformel *Regression:*Kriterium~Prädiktor(en) *Varianzanalyse:*AV~UV(s als Faktor(en)) Modell-Term Der Modell-Term auf der rechten Seite der Tilde wird dabei aus einer Reihe von Variablen und Kombinationsoperatoren zusammengesetzt. Zuerst etwas unintuitiv sind diese Operatoren im normalen R-Kontext mit anderen Bedeutungen belegt, in formulas funktionieren sie aber so nicht Die Operatoren sind die folgenden: Operator übliche Bedeutung Bedeutung in formulas Addition Vorhersageterm hinzufügen Subtraktion Vorhersageterm ausschließen &lt;A&gt; : &lt;B&gt; Sequenz Interaktion AxB &lt;A&gt; * &lt;B&gt; Multiplikation Effekt von A, B und AxB Anhand von einer Reihe von Beispielen wird die Formulierung deutlich, dafür führen wir noch kurz eine Hand voll Notationen ein, die meisten davon sind wahrscheinlich nicht überraschend: Abkürzung Bedeutung \\(H_0\\) Nullhypothese eines statistischen Tests \\(H_1\\) Alternativhypothese eines statistischen Tests \\(UV\\) unabhängige Variable \\(AV\\) abhängige Variable \\(X_i / Y_i\\) numerische (Zufalls-) Variable \\(F_i\\) kategoriale Varable (Faktor) Regressionsmodelle Y ~ X1: einfache lineare Regression von Y auf X1 Y ~ X1 + X2: multiple lineare Regression von Y auf X1 und X2 Y ~ X1+X2+X1:X2: multiple lineare Regression von Y auf X1 und X2 sowie auf den Interaktionsterm von X1 und X2 Y ~ X1*X2: multiple lineare Regression von Y auf X1 und X2 sowie auf den Interaktionsterm von X1 und X2 Varianzanalytische Modelle Y ~ F1: einfaktorielle Varianzanalyse Y ~ F1 + F2 + F1:F2: zweifaktorielle Varianzanalyse mit beiden Haupteffekten und der Interaktion Y ~ F1 * F2: auch zweifaktorielle Varianzanalyse mit beiden Haupteffekten und der Interaktion Y ~ X1 + F1: Kovarianzanalyse mit Kovariate X1 und Faktor F1 Innerhalb einer Modellformel können die Terme selbst das Ergebnis der Anwendung von Funktionen auf Variablen sein: \\[\\texttt{log}(Y) \\sim \\texttt{scale}(X)\\] Wenn wir die für die Formulierung genutzten Operatoren für arithmetische Operationen in der Modellformel verwenden wollen, müssen sie mit I() eingeschlossen werden um den Kontext klarzumachen: \\[Y \\sim \\texttt{I}(2*X)\\] Aufgabe Welche Hypothese(n) pass(t/en) zu folgender Modellformel: IQ ~ Geschlecht + Raucher A: Es gibt einen Unterschied zwischen der Intelligenz von Rauchern und Nichtrauchern und zwischen der von Frauen und Männern. B: Es gibt einen Unterschied zwischen der Intelligenz von Rauchern und Nichtrauchern und zwischen der von Frauen und Männern sowie einen Unterschied in der Intelligenz zwischen Rauchern und Nichtrauchern, der sich in der Ausprägung zwischen den Geschlechtern unterscheidet. C: Es gibt einen Unterschied in der Intelligenz zwischen Rauchern und Nichtrauchern, der sich in der Ausprägung zwischen den Geschlechtern unterscheidet. D: Es gibt einen Zusammenhang zwischen Rauchen und Geschlecht auf der einen und Intelligenz auf der anderen Seite. Lösung A ist richtig. "],["einfache-lineare-zusammenhänge.html", "Einfache lineare Zusammenhänge Test auf Korrelation Einfache lineare Regression Regressionsanalyse", " Einfache lineare Zusammenhänge Datensatz Für die Tests auf linearen Zusammenhänge werden wir den Datensatz df_wide mit den folgenden Variablen benutzen: Variable Inhalt group Treatment-Gruppe pre_skill motorischer Skill vor dem Treatment post_skill motorischer Skill nach dem Treatment hawie_iq Intelligenz-Quotient aus HAWIE hawie_wahr_log Skalenwert Wahrnehmungsgebundenes logisches Denken aus HAWIE Test auf Korrelation Test-Hintergrund Die empirische Korrelation zweier gemeinsam normalverteilter Variablen lässt sich daraufhin testen, ob sie mit der \\(H_0\\) ‘kein linearer Zusammenhang’ (\\(\\text{H}_{0}:\\rho_{X,Y} = 0\\)) verträglich ist. Dabei wird genutzt, dass bei Multinormalverteilung und Unkorreliertheit der Variablen \\(X\\) und \\(Y\\) die Teststatistik \\(t_r = r_{x,y} \\sqrt{{n-2}\\over{1-r_{x,y}}^2}\\) \\(t-\\)verteilt ist, mit \\(n-2\\) Freiheitsgraden. Man testet also die Teststatistik \\(t\\) gegen die \\(t_{n-2}\\) -Verteilung. Ist der Test signifikant, wird die \\(H_1\\) angenommen, also dass die ‘wahre’ Korrelation zwischen \\(X\\) und \\(Y\\) ungleich 0 ist. Gerichtete Hypothesen lassen sich analog testen. Test auf Korrelation in R Man kann den Test in R mit Vektoren als Eingabe… cor.test(df_wide$hawie_iq, df_wide$hawie_wahr_log) ## ## Pearson&#39;s product-moment correlation ## ## data: df_wide$hawie_iq and df_wide$hawie_wahr_log ## t = 7.0471, df = 48, p-value = 6.228e-09 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.5424104 0.8272449 ## sample estimates: ## cor ## 0.7130958 … und mit Modellformel als Eingabe aufrufen. cor.test(~ hawie_iq + hawie_wahr_log, data = df_wide) ## ## Pearson&#39;s product-moment correlation ## ## data: hawie_iq and hawie_wahr_log ## t = 7.0471, df = 48, p-value = 6.228e-09 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.5424104 0.8272449 ## sample estimates: ## cor ## 0.7130958 Das alternative-Argument bietet die Möglichkeit, die Richtung des Signifikanztests anzugeben. Dabei steht 'greater' für einen rechtsseitigen, 'lesser' für einen linksseitigen und der Standard 'two.sided' für einen zweiseitigen Test. cor.test(~ hawie_iq + hawie_wahr_log, data = df_wide, alternative=&#39;greater&#39;) ## ## Pearson&#39;s product-moment correlation ## ## data: hawie_iq and hawie_wahr_log ## t = 7.0471, df = 48, p-value = 3.114e-09 ## alternative hypothesis: true correlation is greater than 0 ## 95 percent confidence interval: ## 0.5740402 1.0000000 ## sample estimates: ## cor ## 0.7130958 Der Output lässt sich noch ein bisschen schicker mit der tidy-Funktion aus dem broom-Paket darstellen (ist auch im tidyverse enthalten): cor.test(~ hawie_iq + hawie_wahr_log, data = df_wide, alternative=&#39;greater&#39;) %&gt;% broom::tidy() ## # A tibble: 1 x 8 ## estimate statistic p.value parameter conf.low ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.713 7.05 0.00000000311 48 0.574 ## conf.high method ## &lt;dbl&gt; &lt;chr&gt; ## 1 1 Pearson&#39;s product-moment correlation ## alternative ## &lt;chr&gt; ## 1 greater Aufgabe Wie kann ich das Ergebnis interpretieren? cor.test(~ hawie_iq + hawie_wahr_log, data = df_wide, alternative=&#39;greater&#39;) %&gt;% broom::tidy() ## # A tibble: 1 x 8 ## estimate statistic p.value parameter conf.low ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.713 7.05 0.00000000311 48 0.574 ## conf.high method ## &lt;dbl&gt; &lt;chr&gt; ## 1 1 Pearson&#39;s product-moment correlation ## alternative ## &lt;chr&gt; ## 1 greater A: Die Logik-Leitung beeinflusst den IQ signifikant positiv. B: Es gibt keine Korrelation zwischen Logik-Leistung und IQ. C: Die Logik-Leistung und der IQ sind signifikant von Null unterschiedlich korreliert. D: Es gibt einen signifikanten, positiv linearen Zusammenhang zwischen Logik-Leistung und IQ. Lösung C und D könnte man so sagen, D hat aber natürlich mehr Informationsgehalt. Einfache lineare Regression Modellanpassung Bei der einfachen linearen Regression werden anhand der paarweise vorhandenen Daten zweier Variablen X und Y die Parameter a und b der Vorhersagegleichung \\(\\hat{Y} = bX + a\\) so bestimmt, dass die Werte von \\(Y\\) (dem Kriterium) bestmöglich mit der Vorhersage \\(\\hat{Y}\\) aus den Werten von \\(X\\) (dem Prädiktor) übereinstimmen. Als Maß für die Güte der Vorhersage wird die Summe der quadrierten Residuen \\(E = Y - \\hat{Y}\\) , also der Abweichungen von vorhergesagten und Kriteriumswerten herangezogen. Lineare Modelle wie das der Regression lassen sich mit lm() anpassen und so die Parameter a und b schätzen. lm(formula= &lt;Modellformel&gt; , data=&lt;Datensatz&gt;) Ein von lm() zurückgegebenes Objekt stellt ein deskriptives Modell der Daten dar, das in anderen Funktionen weiter verwendet werden kann. Beispiel für deskriptive Modellanpassung Als Beispiel soll die Leistung auf der Skala zum wahrnehmungsgebundenen logischen Denken als Kriterium mit dem IQ als Prädiktor vorhergesagt werden. (fitI &lt;- lm(hawie_wahr_log ~ hawie_iq, data = df_wide)) ## ## Call: ## lm(formula = hawie_wahr_log ~ hawie_iq, data = df_wide) ## ## Coefficients: ## (Intercept) hawie_iq ## -3.2874 0.1301 ggplot(df_wide, aes(x = hawie_iq, y = hawie_wahr_log)) + geom_point() + geom_smooth(formula = y ~ x , method = &#39;lm&#39;,col =&#39;red&#39;,se = F) \\(\\beta\\)-Gewicht Will man statt des \\(b\\)-Gewichtes das standardisierte \\(\\beta\\)-Gewicht angeben, muss in der Modellformel z-transformiert werden. Dafür könenn wir entweder alle Teile der formula scalen: (fitZ &lt;- lm(scale(hawie_wahr_log) ~ scale(hawie_iq), data = df_wide)) ## ## Call: ## lm(formula = scale(hawie_wahr_log) ~ scale(hawie_iq), data = df_wide) ## ## Coefficients: ## (Intercept) scale(hawie_iq) ## -2.284e-16 7.131e-01 Oder wir benutzen die Index-pipe %$% aus dem magrittr-Paket und ein zwischengeschaltetes mutate, um die Skalierung ein bisschen übersichtlicher zu gestalten: library(magrittr) fitZ &lt;- df_wide %&gt;% mutate(hawie_wahr_log = scale(hawie_wahr_log), hawie_iq = scale(hawie_iq)) %$% lm(hawie_wahr_log ~ hawie_iq) fitZ ## ## Call: ## lm(formula = hawie_wahr_log ~ hawie_iq) ## ## Coefficients: ## (Intercept) hawie_iq ## -2.284e-16 7.131e-01 df_wide %&gt;% mutate(hawie_wahr_log = scale(hawie_wahr_log), hawie_iq = scale(hawie_iq)) %&gt;% ggplot(aes(x = scale(hawie_iq), y = scale(hawie_wahr_log))) + geom_point() + geom_smooth(formula = y ~ x , method = &#39;lm&#39;,col =&#39;red&#39;,se = F) weitere Parameter lm() gibt eine Liste zurück, die ein deskriptives Modell der Daten darstellt. R bietet weitere Funktionen um einzelne Parameter dieses Outputs auszulesen. Zum Beispiel: residuals() zum Anzeigen der Residuen, coef() zur Ausgabe der geschätzten Modellparameter und fitted() für die vorhergesagten Werte. residuals(fitZ) ## 1 2 3 4 ## -0.051410121 -0.124304792 2.196986725 0.843014168 ## 5 6 7 8 ## -0.394825317 0.107961353 0.235184545 -1.080533632 ## 9 10 11 12 ## 0.537853351 -0.675313884 -0.704970155 0.813357897 ## 13 14 15 16 ## 0.002918400 0.440286430 0.715790976 0.191946144 ## 17 18 19 20 ## 0.107961353 -0.051410121 -0.040320001 0.035066681 ## 21 22 23 24 ## -1.107697893 -0.267602125 0.007902421 -0.440555728 ## 25 26 27 28 ## -1.758765916 -0.024245860 -0.702478144 0.453868560 ## 29 30 31 32 ## 0.383465899 0.553927492 -0.037827990 0.208020285 ## 33 34 35 36 ## -0.340496797 -0.067484261 0.770119496 -0.294766385 ## 37 38 39 40 ## 0.148707743 0.699716835 0.497106961 1.407605390 ## 41 42 43 44 ## 0.599657903 0.802267777 -0.299750406 0.078305082 ## 45 46 47 48 ## -0.948326419 -0.618493353 -0.805029086 -1.323889897 ## 49 50 ## -0.078574381 -0.599927202 ## attr(,&quot;scaled:center&quot;) ## [1] 10.08 ## attr(,&quot;scaled:scale&quot;) ## [1] 3.009102 Im broom-Paket gibt es außerdem die augment-Funktion, die uns den zum Fitten genutzten Datensatz mit einer Reihe von Zusatzinfos ausgibt. broom::augment(fitZ) ## # A tibble: 50 x 8 ## hawie_wahr_log[,1] hawie_iq[,1] .fitted .resid ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.306 0.501 0.357 -0.0514 ## 2 -0.0266 0.137 0.0977 -0.124 ## 3 0.306 -2.65 -1.89 2.20 ## 4 0.638 -0.287 -0.205 0.843 ## 5 1.30 2.38 1.70 -0.395 ## 6 0.638 0.743 0.530 0.108 ## 7 -0.359 -0.833 -0.594 0.235 ## 8 -2.02 -1.32 -0.940 -1.08 ## 9 -0.359 -1.26 -0.897 0.538 ## 10 -1.36 -0.954 -0.681 -0.675 ## .hat .sigma .cooksd .std.resid ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0251 0.716 0.0000696 -0.0735 ## 2 0.0204 0.716 0.000327 -0.177 ## 3 0.164 0.624 1.12 3.39 ## 4 0.0217 0.705 0.0160 1.20 ## 5 0.136 0.713 0.0282 -0.600 ## 6 0.0313 0.716 0.000387 0.155 ## 7 0.0342 0.715 0.00202 0.338 ## 8 0.0555 0.697 0.0723 -1.57 ## 9 0.0523 0.711 0.0168 0.780 ## 10 0.0386 0.709 0.0190 -0.972 ## # … with 40 more rows fitZ %&gt;% broom::augment() %&gt;% ggplot(aes(x = hawie_iq, y = hawie_wahr_log))+ geom_linerange(aes(ymin = .fitted, ymax = hawie_wahr_log), col = &#39;orange&#39;) + geom_point() + geom_smooth(formula = y ~ x , method = &#39;lm&#39;,col =&#39;red&#39;,se = F) Regressionsanalyse Test-Hintergrund Unter Voraussetzungen von Varianzhomogenität und Normalverteilung der \\(Y\\)-Werte für jeden möglichen Wert von \\(X\\) können Regressionskoeffizienten ähnlich wie Korrelationskoeffizienten auf Unterschiedlichkeit von 0 getestet werden. Dazu wird genutzt, dass der Term \\(t = {{b}\\over{{s_{Y \\cdot X}}\\over {s_X \\sqrt{N-1}}}}\\) \\(t_{N-1}\\)-verteilt ist, wenn das tatsächliche \\(b^*\\) nicht unterschiedlich von 0 ist und für jeden Wert von \\(X\\) \\(Y\\) normalverteilt ist mit \\(\\mu = b^*X + a^*\\) und einer Varianz \\(\\sigma^2\\). Die Nullhypothese ist also \\(\\text{H}_0: b^* = 0\\). Test in R Um zusätzliche Informationen (insbesondere inferenzstatistische Kennwerte) eine mit lm() erstellten Regressions-Modells zu erhalten, kann einfach summary() verwendet werden. fitZ %&gt;% summary() ## ## Call: ## lm(formula = hawie_wahr_log ~ hawie_iq) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.75877 -0.38124 -0.01066 0.45047 2.19699 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.284e-16 1.002e-01 0.000 1 ## hawie_iq 7.131e-01 1.012e-01 7.047 6.23e-09 ## ## (Intercept) ## hawie_iq *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7083 on 48 degrees of freedom ## Multiple R-squared: 0.5085, Adjusted R-squared: 0.4983 ## F-statistic: 49.66 on 1 and 48 DF, p-value: 6.228e-09 Außerdem gibt es auch hier einen hübschen broom-Output, für den wir mit conf.int angeben können, Konfidenzintervalle für die Regressionsgewichte ausgeben lassen zu wollen: fitZ %&gt;% broom::tidy(conf.int = T) ## # A tibble: 2 x 7 ## term estimate std.error statistic ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -2.28e-16 0.100 -2.28e-15 ## 2 hawie_iq 7.13e- 1 0.101 7.05e+ 0 ## p.value conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.00 -0.201 0.201 ## 2 0.00000000623 0.510 0.917 Aufgabe fitZ %&gt;% broom::tidy(conf.int = T) ## # A tibble: 2 x 7 ## term estimate std.error statistic ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -2.28e-16 0.100 -2.28e-15 ## 2 hawie_iq 7.13e- 1 0.101 7.05e+ 0 ## p.value conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.00 -0.201 0.201 ## 2 0.00000000623 0.510 0.917 Wie lässt sich das Ergebnis interpretieren? * A: Höhere IQ-Werte hängen mit höheren Logik-Leistungswerten zusammen. * B: Es gibt keine Korrelation zwischen IQ-Werten und Logik-Leistung. * C: Mit jedem Anstieg des IQ um eine Streuungs-Einheit, steigt die Vorhersage um 0.7130958 Streuungs-Einheiten. * D: Man kann wegen der unzureichenden Berücksichtigung nicht-linearer Zusammenhänge keine Aussage treffen. Lösung C könnte man so sagen. "],["literatur.html", "Literatur", " Literatur Tukey, John W. 1977. Exploratory Data Analysis. Vol. 2. Reading, Mass. "]]
