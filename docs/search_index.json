[["index.html", "EDV2 Vorwort", " EDV2 Max Brede 2022-05-10 Vorwort Dieses mit bookdown erstellte Dokument ist das Skript zur Übung “PSY_B_12-2: Computerunterstützte Datenanalyse II” der CAU zu Kiel. Dieses Skript steht, wo nicht anders angegeben, unter der CC BY-SA 4.0-Lizenz. "],["lehrplan.html", "Lehrplan Semesterplan Übungsformat Lehrziele für jede Sitzung Prüfungsleistung", " Lehrplan Semesterplan Einheit Vorlesung Übungswoche Thema 1 12.4.22 keine Übung Deskriptive Statistik Data Cleaning 2 26.4.22 KW 18 Hilfsmittel für die Inferenzstatistik Lineare Regression I 3 10.5.22 KW 20 Lineare Regression II 4 24.5.22 KW 22 t- Tests einfaktorielle Varianzanalyse 5 7.6.22 KW 24 zweifaktorielle Varianzanalyse 6 28.6.22 KW 26 Kontrasttests 7 5.7.22 keine Übung Klausurvorbereitung Übungsformat Die Übung soll zur Hälfte in 45-minütigen Sitzungen im Vorlesungsformat zur Vorstellung der Funktionen und zur anderen Hälfte als 45-minütige praktische Übung stattfinden. Es wird pro Übungs-Sitzung ein Übungszettel ausgegeben, der mit Hilfe der in der Zugehörigen Vorlesung besprochenen Funktionen bearbeitet werden können soll. Diese Zettel sollen nach der jeweiligen Vorlesung für die Übungen vorbereitet werden, in denen der Zettel dann besprochen und mögliche Fragen geklärt werden. Nach den Übungssitzungen haben die Studierenden dann eine Woche Zeit, zusätzliche Hausaufgaben zu bearbeiten. Eine Ausnahme von diesem Ablauf ist die erste Sitzung, in der organisatorisches und Grundlagen in 90 minütigem Vorlesungsstil besprochen werden sollen. Auch nach dieser Sitzung werden aber Übungszettel und Hausaufgaben ausgegeben. Lehrziele für jede Sitzung Die Studierenden können nach dem Absolvieren der Übung… Einheit 1 Gründe für deskriptive Statistik nennen. für verschiedene Ausgangssituationen entscheiden, welche Darstellungsform angemessen ist. Verfahren zum Umgang mit fehlenden Werten nennen und anwenden. Verfahren zum Umgang mit Ausreißern nennen und anwenden. Einheit 2 R formulas lesen und verwenden. Korrelationsanalysen in R durchführen und die Ergebnisse interpretieren. einfache lineare Regressionen in R durchführen und die Ergebnisse interpretieren. Einheit 3 multiple lineare Regressionen in R durchführen und die Ergebnisse interpretieren. Einheit 4 t-Tests in R durchführen und die Ergebnisse interpretieren. einfaktorielle Varianzanalysen in R durchführen und die Ergebnisse interpretieren. Einheit 5 zweifaktorielle Varianzanalysen in R durchführen und die Ergebnisse interpretieren. Einheit 6 beliebige Linearkontraste in R durchführen und die Ergebnisse interpretieren. paarweise post-hoc t-Tests in R durchführen und die Ergebnisse interpretieren. Einheit 7 erfolgreich an der Klausur teilnehmen. Prüfungsleistung Die Studierenden müssen während des Semesters die nach den Übungssitzungen ausgegebenen Hausaufgaben innerhalb einer Woche sinnvoll bearbeitet abgeben. Mit maximal einer nicht sinnvoll bearbeiteten Serie werden die Studierenden zur Gruppenarbeit am Ende des Semesters zugelassen. "],["deskriptive-statistik-und-data-cleaning.html", "Deskriptive Statistik und Data Cleaning Organisatorisches Deskriptive Statistik Data Cleaning Organisationsformen von Datensätzen", " Deskriptive Statistik und Data Cleaning Organisatorisches Übungsablauf Es wird alle zwei Wochen eine 45-minütige Vorlesung geben und dazu alternierend online-Übungsstunden in mit je einem Kurs. (Eine Ausnahme ist die erste Woche, in der wir nur eine 90-minütige Vorlesung haben.) Übungsablauf Prüfungsleistung Gruppenarbeit(4-5 Personen) über 3 Wochen Termin für die Klausur entweder vor oder mit Anfang in der Klausurphase. Genauere Informationen gibt es über das Olat. Als Zulassung für die Gruppenarbeit ist wieder die sinnvolle Bearbeitung von allen bis auf eine Hausaufgabenserien nötig. Deskriptive Statistik Wozu brauche ich das? Im Gegensatz zur Inferenzstatistik ist das erklärte Ziel der deskriptiven Statistik, (wie der Name schon sagt) beschreibende Aussagen über die vorliegende Stichprobe zu treffen. Wir wollen uns also möglichst genau angucken, wie unsere Stichprobe aussieht. Wozu könnte das gut sein? Gründe für deskriptive Statistik Indikatoren zur externen Validität(Verteilung von Organismusvariablen, Demografie,…) Aussagen über Verteilungseigenschaften Schnell zu erfassende Präsentationen von zentraler Tendenz und Streuungen Hinweise auf ungewöhnliche Werte (Ausreißer, fehlende Werte,…) Übersicht über Effekte und Zusammenhänge, inklusive solcher, die möglicherweise nicht a-priori erwartet wurden deskriptive Statistik Für einen ganz einfachen, schnellen und umfassenden Überblick über die Daten funktioniert die skim-Funktion aus dem skimr-Paket gut: skimr::skim(df) Tab. 1: Data summary Name df Number of rows 12498 Number of columns 5 _______________________ Column type frequency: character 2 logical 1 numeric 2 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace group 0 1 3 3 0 7 0 sex 0 1 1 1 0 2 0 Variable type: logical skim_variable n_missing complete_rate mean count smoker 0 1 0.5 TRU: 6297, FAL: 6201 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist IQ 0 1 100 15 69.71 87.34 100.57 112.61 126.67 ▅▆▇▇▆ age 0 1 40 20 17.85 25.00 32.16 53.63 88.72 ▇▅▁▂▂ Demografie Einfache, schnell zu erfassende Beschreibung der Stichprobe zum Beispiel über eine Tabelle: df %&gt;% count(sex, smoker, group) %&gt;% pivot_wider(names_from = group, values_from = n) ## # A tibble: 4 × 9 ## sex smoker g_1 g_2 g_3 g_4 g_6 g_7 ## &lt;chr&gt; &lt;lgl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 f FALSE 373 472 609 384 430 482 ## 2 f TRUE 375 447 634 413 451 474 ## 3 m FALSE 361 441 579 394 442 464 ## 4 m TRUE 397 469 617 358 424 437 ## g_8 ## &lt;int&gt; ## 1 390 ## 2 395 ## 3 380 ## 4 406 Aber vielleicht ein bisschen übersichtlicher in einer Grafik: df %&gt;% count(sex, smoker, group) %&gt;% ggplot(aes(x = group, fill = smoker, y = n)) + geom_col(position = &#39;dodge&#39;) + facet_wrap(~sex) + labs(x = &#39;Group&#39;, y = &#39;Count&#39;, fill = &#39;Smoker&#39;) Man sieht auf einen Blick, dass Geschlecht und Raucher ungefähr gleich aufgeteilt wurden, die Gruppen aber wesentlich unterschiedliche Größen aufweisen! Aussagen über Verteilungseigenschaften In der skim-Ausgabe haben wir ja schon sehen können, dass keine fehlenden Werte vorliegen (n_missing war 0). Wir könnten uns aber noch die Frage stellen, ob Extremwerte in den Gruppen auftauchen, außerdem wollen wir möglichst übersichtlich unsere Verteilungseigenschaften präsentieren. Das hilft uns zum Einen, um einen besseren Überblick über die Daten zu erhalten, die wir auswerten wollen, zum Anderen hilft es bei einem späteren Bereicht der Leserin, unsere Statistik einzuschätzen. Dazu können wir uns entweder eine Tabelle mit den Verteilungsparametern der numerischen Variablen ausgeben lassen: df %&gt;% pivot_longer(where(is.numeric), names_to = &#39;variable&#39;) %&gt;% group_by(variable) %&gt;% summarise(across( value, list( mean = ~ mean(.), sd = ~ sd(.), min = ~ min(.), q1 = ~ quantile(., .25), median = ~ median(.), q3 = ~ quantile(., .75), max = ~ max(.) ), .names = &#39;{.fn}&#39; )) %&gt;% mutate(across(where(is.numeric), ~round(.,2))) ## # A tibble: 2 × 8 ## variable mean sd min q1 median q3 max ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 age 40 20 17.8 25 32.2 53.6 88.7 ## 2 IQ 100 15 69.7 87.3 101. 113. 127. Oder, wieder ein bisschen übersichtlicher, in einem Diagramm. So könnten wir die ganzen Infos gerade zum Beispiel in einem Boxplot mit eingezeichneten Mittelwerten und Streuungsbalken darstellen: df %&gt;% pivot_longer(where(is.numeric), names_to = &#39;variable&#39;) %&gt;% ggplot(aes(x = &#39;&#39;, y = value)) + geom_violin(fill = &#39;lightgrey&#39;, color = NA) + geom_boxplot(width = .25) + stat_summary(fun = mean, fun.min = function(x)mean(x) - sd(x), fun.max = function(x)mean(x) + sd(x), color = &#39;red&#39;) + facet_wrap(~variable, scales = &#39;free&#39;) + labs(x = &#39;&#39;, y = &#39;&#39;) Das Alter ist eindeutig schief verteilt, der IQ dafür mehrgipflig. Nach der von Tukey aufgestellten Regel (Tukey 1977) haben wir auch keine Ausreißer (dazu auch später mehr). Darstellung von Effekten und Zusammenhängen Unterschiede Da wir eine Reihe von Gruppierungsvariablen haben, könnte der erste Impuls sein, sich die Variablen nach diesen Gruppen aufgeteilt darstellen zu lassen, um mögliche Gruppenunterschiede zu verdeutlichen. Auch hier können wir uns Tabellen erstellen: df %&gt;% group_by(smoker, group, sex) %&gt;% summarise(across(where(is.numeric), .fns = list(mean = ~mean(.), sd = ~sd(.), n = ~n()))) %&gt;% mutate(across(where(is.numeric), ~round(.,2))) ## # A tibble: 28 × 9 ## # Groups: smoker, group [14] ## smoker group sex IQ_mean IQ_sd IQ_n age_mean ## &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 FALSE g_1 f 99.0 13.9 373 31.8 ## 2 FALSE g_1 m 98.4 13.9 361 30.6 ## 3 FALSE g_2 f 98.9 19.3 472 52.0 ## 4 FALSE g_2 m 97.4 19.0 441 53.0 ## 5 FALSE g_3 f 102. 14.0 609 42.5 ## 6 FALSE g_3 m 102. 13.7 579 41.7 ## 7 FALSE g_4 f 101. 13.8 384 40.8 ## 8 FALSE g_4 m 99.3 13.1 394 42.0 ## 9 FALSE g_6 f 99.3 14.6 430 34.8 ## 10 FALSE g_6 m 101. 15.4 442 35.9 ## age_sd age_n ## &lt;dbl&gt; &lt;dbl&gt; ## 1 17.3 373 ## 2 15.3 361 ## 3 27.9 472 ## 4 27.7 441 ## 5 18.2 609 ## 6 17.4 579 ## 7 19.3 384 ## 8 20.3 394 ## 9 14.6 430 ## 10 14.9 442 ## # … with 18 more rows Oder Plots erstellen um die möglichen Unterschiede darzustellen: df %&gt;% pivot_longer(cols = where(is.numeric), names_to = &#39;variable&#39;) %&gt;% group_by(variable, smoker, sex, group) %&gt;% summarise(m = mean(value), sem = sqrt(var(value)/n()), upper = m + sem, lower = m - sem) %&gt;% ggplot(aes(x = group, y = m, fill = smoker)) + geom_col(position = &#39;dodge&#39;) + geom_errorbar(aes(ymin = lower, ymax = upper), position = &#39;dodge&#39;)+ facet_grid( variable ~ sex , scales = &#39;free&#39;) Das wird zwar ein bisschen unübersichtlich (wenn man das wirklich sinnvoll betreiben wollen würde sollte man sich Gedanken dazu machen, welche Variablen tatsächlich von Relevanz sind), man könnte aber zu dem Schluss kommen dass die IQs relativ ähnlich sind, die Altersgruppen aber nicht. Zusammenhänge Zuletzt wollen wir noch gucken, ob in den Daten irgendwelche (linearen) Zusammenhänge direkt ersichtlich sind. Dazu können wir zuerst Korrelationen berechnen, zum Beispiel einmal für die gesamte Stichprobe und einmal für die Untergruppen: library(magrittr) df %$% cor(age, IQ) ## [1] 0.06659135 df %&gt;% group_by(group) %&gt;% summarise(r = cor(age, IQ)) ## # A tibble: 7 × 2 ## group r ## &lt;chr&gt; &lt;dbl&gt; ## 1 g_1 0.0540 ## 2 g_2 0.00747 ## 3 g_3 0.110 ## 4 g_4 0.0952 ## 5 g_6 0.111 ## 6 g_7 0.139 ## 7 g_8 0.0636 Hier ist so weit nichts auffällig. Ein letzter zu überprüfender Aspekt sind die nicht-linearen Zusammenhänge, zum Beispiel über angemessene Plots. Dies können wir zum Einen für die Untergruppen überprüfen wollen: df %&gt;% ggplot(aes(x = IQ, y = age, color = group)) + geom_point() + facet_grid(smoker ~ sex) Zum Anderen für die gesamte Stichprobe: df %&gt;% ggplot(aes(IQ, age, color = group)) + geom_point(size = .001) + scale_color_grey() Abb. 1: Original-Comic von xkcd Data Cleaning Umgang mit fehlenden Werten NAs sind das in R zur Codierung von fehlenden Werten genutzte Datenformat. Sie können in Vektoren (und damit auch tibble-Spalten) jeden Datenformats auftreten: c(T,NA,F) ## [1] TRUE NA FALSE c(1,NA,2) ## [1] 1 NA 2 c(&#39;a&#39;,NA,&#39;b&#39;) ## [1] &quot;a&quot; NA &quot;b&quot; Wenn wir versuche mit einem Vektor zu rechnen, der NAs beinhaltet, können wir auf Probleme stoßen: aVector &lt;- c(NA,1,2,5,NA,6,8) length(aVector[aVector &gt; 3]) ## [1] 5 mean(aVector) ## [1] NA Mit der is.na()-Funktion können wir uns einen logischen Vektor ausgeben lassen, der fehlende Werte codiert. Den können wir dann wie gewohnt benutzen: sum(is.na(aVector)) ## [1] 2 any(is.na(aVector)) ## [1] TRUE fehlende Werte und einfache Kennwerte Die meisten Funktionen im base R Umfang haben ein na.rm-Argument mit dem wir fehlende Werte von Berechnungen ausschließen können. Das kann an vielen Stellen schon eine sinnvolle Lösung sein, zum Beispiel wenn wir die Infos über die Anzahl fehlender Werte nicht verlieren wollen. Das könnte dann zum Beispiel so aussehen: mean(aVector) ## [1] NA mean(aVector, na.rm = T) ## [1] 4.4 Dieses Argument können wir auch in die gewohnten Pipelines einsetzen. Als Beispiel nehmen wir diesen kleinen (unrealistisch unvollständigen) Datensatz df_2: (df_2 &lt;- read_csv(&#39;data/small_nas.csv&#39;)) ## # A tibble: 12 × 4 ## VP group t_1 t_2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 3.66 -1.53 ## 2 2 2 NA 3.32 ## 3 3 3 2.23 NA ## 4 4 4 4.82 4.14 ## 5 5 1 0.142 0.537 ## 6 6 2 1.86 5.04 ## 7 7 3 NA 1.46 ## 8 8 4 3.60 3.50 ## 9 9 1 0.353 NA ## 10 10 2 NA NA ## 11 11 3 3.79 2.57 ## 12 12 4 5.37 4.95 Wir könnten jetzt die Pipeline für die Gruppenunterschiede von eben nochmal benutzen, aber um eine Angabe zur Anzahl der fehlenden Werte ergänzen: df_2 %&gt;% group_by(group) %&gt;% summarise(across(matches(&#39;t_&#39;), .fns = list(mean = ~mean(., na.rm = T), sd = ~sd(., na.rm = T), n = ~n(), missing = ~sum(is.na(.))))) %&gt;% mutate(across(where(is.numeric), ~round(.,2))) ## # A tibble: 4 × 9 ## group t_1_mean t_1_sd t_1_n t_1_missing t_2_mean ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.39 1.97 3 0 -0.5 ## 2 2 1.86 NA 3 2 4.18 ## 3 3 3.01 1.1 3 1 2.01 ## 4 4 4.6 0.91 3 0 4.2 ## t_2_sd t_2_n t_2_missing ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.46 3 1 ## 2 1.22 3 1 ## 3 0.79 3 1 ## 4 0.73 3 0 NA-Bereinigung von Datensätzen Vor statistischen Auswertungen kann es aber einfacher sein, den Datensatz komplett von fehlenden Werten zu bereinigen. Je nach dem Fall und der Person die man fragt, gibt es verschiedene Vorgehensweisen. Wir gucken uns hier genauer den fallweisen Ausschluss und das ersetzen durch Werte der zentralen Tendenz an. Die Entscheidung für das Auffüllen oder das Ausschließen muss von Fall zu Fall gefällt werden! Wenn wir zum Beispiel unseren df_2 nochmal angucken, fehlt ein Viertel der Werte. Hier die Fälle aufzufüllen und so zu tun als würde man mit 133% der Werte arbeiten, die tatsächlich vorlagen, ist offensichtlich schwierig. Gleichzeitig wird oft das Argument vorgebracht, dass insbesondere diejenigen Versuchspersonen, die in bestimmten Bedingungen keine Antwort produziert haben ein wichtiger Teil der Stichprobe sind und das Auslassen an sich als Form der Antwort betrachtet werden kann. Wenn wir diese Versuchspersonen ausschließen, verzerren wir nach diesem Argument also systematisch unsere Stichprobe. Wichtig ist also vor jeder Bereinigung, Überlegungen darüber anzustellen, was im gegebenen Fall gerade die angemessenste Lösung darstellt. Bei unserem Datensatz df_2 sind beide Methoden nicht wirklich gut, der Datensatz ist aber auch extrem. Fallweiser Ausschluss Die radikalste Methode ist der Fallweise Ausschluss, also der Ausschluss aller Eintragungen einer Versuchsperson, die mindestens einen fehlenden Wert vorliegen hat. Als Erinnerung, hier nochmal unser Datensatz df_2: VP group t_1 t_2 1 1 3.6612191 -1.5337696 2 2 NA 3.3223479 3 3 2.2250924 NA 4 4 4.8187796 4.1387727 5 1 0.1419190 0.5373972 6 2 1.8635821 5.0435449 7 3 NA 1.4552116 8 4 3.6001703 3.5013055 9 1 0.3530444 NA 10 2 NA NA 11 3 3.7877141 2.5676322 12 4 5.3706695 4.9490249 Wir müssten also die Versuchspersonen ausschließen. Die einfachste Variante dafür ist, den Datensatz ins wide-Format zu überführen (wie er es in unserem Fall schon vorliegt) und mit drop_na diejenigen Zeilen auszuschließen, die fehlende Werte beinhalten: df_2 %&gt;% drop_na() ## # A tibble: 7 × 4 ## VP group t_1 t_2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 3.66 -1.53 ## 2 4 4 4.82 4.14 ## 3 5 1 0.142 0.537 ## 4 6 2 1.86 5.04 ## 5 8 4 3.60 3.50 ## 6 11 3 3.79 2.57 ## 7 12 4 5.37 4.95 Ersetzen fehlender Werte Statt radikal alle Fälle auszuschließen, die mindestens einen fehlenden Wert beinhalten, gibt es auch Ansätze, diese aufzufüllen. Gängige Verfahren hier sind die fehlenden Werte hypothesenunabhängig (also nicht gruppenweise) mit dem (getrimmten) Mittelwert, dem Median oder dem Modus der Gesamtstichprobe aufzufüllen. Die Umsetzung in R sieht dann immer gleich aus, die einzige Änderung findet im Kennwert statt, den man zur Ergänzung wählt. Hier mal ein Beispiel mit dem getrimmten Mittelwert: df_2 %&gt;% mutate(across(where(is.numeric), ~replace_na(., mean(., na.rm=T, trim = .05)))) ## # A tibble: 12 × 4 ## VP group t_1 t_2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 3.66 -1.53 ## 2 2 2 2.87 3.32 ## 3 3 3 2.23 2.66 ## 4 4 4 4.82 4.14 ## 5 5 1 0.142 0.537 ## 6 6 2 1.86 5.04 ## 7 7 3 2.87 1.46 ## 8 8 4 3.60 3.50 ## 9 9 1 0.353 2.66 ## 10 10 2 2.87 2.66 ## 11 11 3 3.79 2.57 ## 12 12 4 5.37 4.95 Umgang mit Ausreißern Ausreißerbereinigung sind ein komplexes Thema, über das viel diskutiert werden kann und auch muss. Da wir uns hier aber im Rahmen einer praktischen Übung befinden sparen wir uns das und nutzen die weit verbreitete Regel, die Tukey (1977) formuliert hat: Diejenigen Werte sind als Ausreißer zu betrachten, die außerhalb des Intervals \\[Q_1 - 1.5 \\cdot \\text{IQR} \\leq x \\leq Q_3 + 1.5 \\cdot \\text{IQR}\\] liegen. Diese Regel ist auch der in geom_boxplot implementierte Standard, den wir ja auch schon zumindest vom Sehen kennen. Die Frage ist nun, was wir mit eventuell detektierten Ausreißern machen. Dafür betrachten wir den folgenden Datensatz df_3: df_3 %&gt;% pivot_longer(cols = everything()) %&gt;% ggplot(aes(y = value, x = &#39;&#39;)) + geom_boxplot() + geom_dotplot(binaxis = &#39;y&#39;, stackdir = &#39;center&#39;, alpha = .5, fill = &#39;red&#39;, dotsize = .5) + facet_wrap(~name,scales = &#39;free_y&#39;) Auch hier können wir wieder zum radikalen Ausschluss greifen und den Datensatz einfach danach filtern, dass unsere Variablen zwischen den “Tukey Fences” liegen: df_3 %&gt;% filter(between(t_1, quantile(t_1,.25) - 1.5 * IQR(t_1), quantile(t_1,.75) + 1.5 * IQR(t_1)) &amp; between(t_2, quantile(t_2,.25) - 1.5 * IQR(t_2), quantile(t_2,.75) + 1.5 * IQR(t_2))) %&gt;% pivot_longer(cols = everything()) %&gt;% ggplot(aes(y = value, x = &#39;&#39;)) + geom_boxplot() + geom_dotplot(binaxis = &#39;y&#39;, stackdir = &#39;center&#39;, alpha = .5, fill = &#39;red&#39;, dotsize = .5) + facet_wrap(~name,scales = &#39;free_y&#39;) Oder wir ‘winsorieren’ unsere Ausreißer, indem wir sie durch den Wert der jeweiligen Grenze ersetzen. Wir sagen also, dass diejenigen Werte, die außerhalb der Fences liegen auf den Wert der jeweiligen Grenze gesetzt werden sollen: df_3 %&gt;% mutate(across(everything(), ~ifelse(. &gt; quantile(.,.75) + 1.5 * IQR(.), quantile(.,.75) + 1.5 * IQR(.), .)), across(everything(), ~ifelse(. &lt; quantile(.,.25) - 1.5 * IQR(.), quantile(.,.25) - 1.5 * IQR(.), .))) %&gt;% pivot_longer(cols = everything()) %&gt;% ggplot(aes(y = value, x = &#39;&#39;)) + geom_boxplot() + geom_dotplot(binaxis = &#39;y&#39;, stackdir = &#39;center&#39;, alpha = .5, fill = &#39;red&#39;, dotsize = .5) + facet_wrap(~name,scales = &#39;free_y&#39;) Organisationsformen von Datensätzen long vs. wide format long-format Name RT Bedingung Snake Müller 2624 1 Snake Müller 3902 2 Snake Müller 6293 3 Vera Baum 1252 1 Vera Baum 2346 2 Vera Baum 4321 3 wide-format Name RT_1 RT_2 RT_3 Snake Müller 2624 3902 6293 Vera Baum 1252 2346 4321 Die pivot-Funktionen pivot_longer und pivot_wider bieten die Möglichkeit, einen Datensatz von einem in das andere Format zu konvertieren. longFormat ## Name RT Bedingung ## 1 Snake Müller 2624 1 ## 2 Snake Müller 3902 2 ## 3 Snake Müller 6293 3 ## 4 Vera Baum 1252 1 ## 5 Vera Baum 2346 2 ## 6 Vera Baum 4321 3 long to wide wideFormat &lt;- longFormat %&gt;% pivot_wider(names_from = &#39;Bedingung&#39;, values_from = &#39;RT&#39;, names_prefix = &#39;RT_&#39;) wideFormat ## # A tibble: 2 × 4 ## Name RT_1 RT_2 RT_3 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Snake Müller 2624 3902 6293 ## 2 Vera Baum 1252 2346 4321 wide to long longFormat &lt;- wideFormat %&gt;% pivot_longer(cols = -1, names_prefix = &#39;RT_&#39;, names_to = &#39;Bedingung&#39;, values_to = &#39;RT&#39;) longFormat ## # A tibble: 6 × 3 ## Name Bedingung RT ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Snake Müller 1 2624 ## 2 Snake Müller 2 3902 ## 3 Snake Müller 3 6293 ## 4 Vera Baum 1 1252 ## 5 Vera Baum 2 2346 ## 6 Vera Baum 3 4321 Literatur "],["hilfsmittel-für-die-inferenzstatistik.html", "Hilfsmittel für die Inferenzstatistik Hilfsmittel für die Inferenzstatistik", " Hilfsmittel für die Inferenzstatistik Hilfsmittel für die Inferenzstatistik Modellterme Alle inferenzstatistischen Verfahren im base-R-Umfang und viele andere aus Zusatzpaketen nutzen die sogenannte Formelschreibweise um Modelle zu definieren. Am Anfang ist die Syntax ein bisschen ungewohnt, am Ende resultiert aus dieser Schreibweise aber eine sehr übersichtliche und schnell erfassbare Modell-Formulierung. Die Formulierung folgt dabei grundsätzlich dem folgenden System, das sich am Besten analog zu einer mathematischen Funktionsgleichung vorgestellt werden kann. Da das = aber schon für Zuweisungen belegt ist, wird es in formula-Schreibweise durch eine Tilde (~) ersetzt: Tab. 2: modellierte Variable(n)~Modellformel *Regression:*Kriterium~Prädiktor(en) *Varianzanalyse:*AV~UV(s als Faktor(en)) Modell-Term Der Modell-Term auf der rechten Seite der Tilde wird dabei aus einer Reihe von Variablen und Kombinationsoperatoren zusammengesetzt. Zuerst etwas unintuitiv sind diese Operatoren im normalen R-Kontext mit anderen Bedeutungen belegt, in formulas funktionieren sie aber so nicht Die Operatoren sind die folgenden: Operator übliche Bedeutung Bedeutung in formulas Addition Vorhersageterm hinzufügen Subtraktion Vorhersageterm ausschließen &lt;A&gt; : &lt;B&gt; Sequenz Interaktion AxB &lt;A&gt; * &lt;B&gt; Multiplikation Effekt von A, B und AxB Anhand von einer Reihe von Beispielen wird die Formulierung deutlich, dafür führen wir noch kurz eine Hand voll Notationen ein, die meisten davon sind wahrscheinlich nicht überraschend: Abkürzung Bedeutung \\(H_0\\) Nullhypothese eines statistischen Tests \\(H_1\\) Alternativhypothese eines statistischen Tests \\(UV\\) unabhängige Variable \\(AV\\) abhängige Variable \\(X_i / Y_i\\) numerische (Zufalls-) Variable \\(F_i\\) kategoriale Varable (Faktor) Regressionsmodelle Y ~ X1: einfache lineare Regression von Y auf X1 Y ~ X1 + X2: multiple lineare Regression von Y auf X1 und X2 Y ~ X1+X2+X1:X2: multiple lineare Regression von Y auf X1 und X2 sowie auf den Interaktionsterm von X1 und X2 Y ~ X1*X2: multiple lineare Regression von Y auf X1 und X2 sowie auf den Interaktionsterm von X1 und X2 Varianzanalytische Modelle Y ~ F1: einfaktorielle Varianzanalyse Y ~ F1 + F2 + F1:F2: zweifaktorielle Varianzanalyse mit beiden Haupteffekten und der Interaktion Y ~ F1 * F2: auch zweifaktorielle Varianzanalyse mit beiden Haupteffekten und der Interaktion Y ~ X1 + F1: Kovarianzanalyse mit Kovariate X1 und Faktor F1 Innerhalb einer Modellformel können die Terme selbst das Ergebnis der Anwendung von Funktionen auf Variablen sein: \\[\\texttt{log}(Y) \\sim \\texttt{scale}(X)\\] Wenn wir die für die Formulierung genutzten Operatoren für arithmetische Operationen in der Modellformel verwenden wollen, müssen sie mit I() eingeschlossen werden um den Kontext klarzumachen: \\[Y \\sim \\texttt{I}(2*X)\\] Aufgabe Welche Hypothese(n) pass(t/en) zu folgender Modellformel: IQ ~ Geschlecht + Raucher A: Es gibt einen Unterschied zwischen der Intelligenz von Rauchern und Nichtrauchern und zwischen der von Frauen und Männern. B: Es gibt einen Unterschied zwischen der Intelligenz von Rauchern und Nichtrauchern und zwischen der von Frauen und Männern sowie einen Unterschied in der Intelligenz zwischen Rauchern und Nichtrauchern, der sich in der Ausprägung zwischen den Geschlechtern unterscheidet. C: Es gibt einen Unterschied in der Intelligenz zwischen Rauchern und Nichtrauchern, der sich in der Ausprägung zwischen den Geschlechtern unterscheidet. D: Es gibt einen Zusammenhang zwischen Rauchen und Geschlecht auf der einen und Intelligenz auf der anderen Seite. Lösung A ist richtig. "],["einfache-lineare-zusammenhänge.html", "Einfache lineare Zusammenhänge Test auf Korrelation Einfache lineare Regression Regressionsanalyse", " Einfache lineare Zusammenhänge Datensatz Für die Tests auf linearen Zusammenhänge werden wir den Datensatz df_wide mit den folgenden Variablen benutzen: Variable Inhalt group Treatment-Gruppe pre_skill motorischer Skill vor dem Treatment post_skill motorischer Skill nach dem Treatment hawie_iq Intelligenz-Quotient aus HAWIE hawie_wahr_log Skalenwert Wahrnehmungsgebundenes logisches Denken aus HAWIE Test auf Korrelation Test-Hintergrund Die empirische Korrelation zweier gemeinsam normalverteilter Variablen lässt sich daraufhin testen, ob sie mit der \\(H_0\\) ‘kein linearer Zusammenhang’ (\\(\\text{H}_{0}:\\rho_{X,Y} = 0\\)) verträglich ist. Dabei wird genutzt, dass bei Multinormalverteilung und Unkorreliertheit der Variablen \\(X\\) und \\(Y\\) die Teststatistik \\(t_r = r_{x,y} \\sqrt{{n-2}\\over{1-r_{x,y}}^2}\\) \\(t-\\)verteilt ist, mit \\(n-2\\) Freiheitsgraden. Man testet also die Teststatistik \\(t\\) gegen die \\(t_{n-2}\\) -Verteilung. Ist der Test signifikant, wird die \\(H_1\\) angenommen, also dass die ‘wahre’ Korrelation zwischen \\(X\\) und \\(Y\\) ungleich 0 ist. Gerichtete Hypothesen lassen sich analog testen. Test auf Korrelation in R Man kann den Test in R mit Vektoren als Eingabe… cor.test(df_wide$hawie_iq, df_wide$hawie_wahr_log) ## ## Pearson&#39;s product-moment correlation ## ## data: df_wide$hawie_iq and df_wide$hawie_wahr_log ## t = 8.0781, df = 48, p-value = 1.678e-10 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.6094958 0.8564580 ## sample estimates: ## cor ## 0.7590665 … und mit Modellformel als Eingabe aufrufen. cor.test(~ hawie_iq + hawie_wahr_log, data = df_wide) ## ## Pearson&#39;s product-moment correlation ## ## data: hawie_iq and hawie_wahr_log ## t = 8.0781, df = 48, p-value = 1.678e-10 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.6094958 0.8564580 ## sample estimates: ## cor ## 0.7590665 Das alternative-Argument bietet die Möglichkeit, die Richtung des Signifikanztests anzugeben. Dabei steht 'greater' für einen rechtsseitigen, 'lesser' für einen linksseitigen und der Standard 'two.sided' für einen zweiseitigen Test. cor.test(~ hawie_iq + hawie_wahr_log, data = df_wide, alternative=&#39;greater&#39;) ## ## Pearson&#39;s product-moment correlation ## ## data: hawie_iq and hawie_wahr_log ## t = 8.0781, df = 48, p-value = 8.392e-11 ## alternative hypothesis: true correlation is greater than 0 ## 95 percent confidence interval: ## 0.6375781 1.0000000 ## sample estimates: ## cor ## 0.7590665 Der Output lässt sich noch ein bisschen schicker mit der tidy-Funktion aus dem broom-Paket darstellen (ist auch im tidyverse enthalten): cor.test(~ hawie_iq + hawie_wahr_log, data = df_wide, alternative=&#39;greater&#39;) %&gt;% broom::tidy() ## # A tibble: 1 × 8 ## estimate statistic p.value parameter conf.low ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.759 8.08 8.39e-11 48 0.638 ## conf.high method ## &lt;dbl&gt; &lt;chr&gt; ## 1 1 Pearson&#39;s product-moment correlation ## alternative ## &lt;chr&gt; ## 1 greater Aufgabe Wie kann ich das Ergebnis interpretieren? cor.test(~ hawie_iq + hawie_wahr_log, data = df_wide, alternative=&#39;greater&#39;) %&gt;% broom::tidy() ## # A tibble: 1 × 8 ## estimate statistic p.value parameter conf.low ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.759 8.08 8.39e-11 48 0.638 ## conf.high method ## &lt;dbl&gt; &lt;chr&gt; ## 1 1 Pearson&#39;s product-moment correlation ## alternative ## &lt;chr&gt; ## 1 greater A: Die Logik-Leitung beeinflusst den IQ signifikant positiv. B: Es gibt keine Korrelation zwischen Logik-Leistung und IQ. C: Die Logik-Leistung und der IQ sind signifikant von Null unterschiedlich korreliert. D: Es gibt einen signifikanten, positiv linearen Zusammenhang zwischen Logik-Leistung und IQ. Lösung C und D könnte man so sagen, D hat aber natürlich mehr Informationsgehalt. Einfache lineare Regression Modellanpassung Bei der einfachen linearen Regression werden anhand der paarweise vorhandenen Daten zweier Variablen X und Y die Parameter a und b der Vorhersagegleichung \\(\\hat{Y} = bX + a\\) so bestimmt, dass die Werte von \\(Y\\) (dem Kriterium) bestmöglich mit der Vorhersage \\(\\hat{Y}\\) aus den Werten von \\(X\\) (dem Prädiktor) übereinstimmen. Als Maß für die Güte der Vorhersage wird die Summe der quadrierten Residuen \\(E = Y - \\hat{Y}\\) , also der Abweichungen von vorhergesagten und Kriteriumswerten herangezogen. Lineare Modelle wie das der Regression lassen sich mit lm() anpassen und so die Parameter a und b schätzen. lm(formula= &lt;Modellformel&gt; , data=&lt;Datensatz&gt;) Ein von lm() zurückgegebenes Objekt stellt ein deskriptives Modell der Daten dar, das in anderen Funktionen weiter verwendet werden kann. Beispiel für deskriptive Modellanpassung Als Beispiel soll die Leistung auf der Skala zum wahrnehmungsgebundenen logischen Denken als Kriterium mit dem IQ als Prädiktor vorhergesagt werden. (fitI &lt;- lm(hawie_wahr_log ~ hawie_iq, data = df_wide)) ## ## Call: ## lm(formula = hawie_wahr_log ~ hawie_iq, data = df_wide) ## ## Coefficients: ## (Intercept) hawie_iq ## -6.7909 0.1742 ggplot(df_wide, aes(x = hawie_iq, y = hawie_wahr_log)) + geom_point() + geom_smooth(formula = y ~ x , method = &#39;lm&#39;,col =&#39;red&#39;,se = F) \\(\\beta\\)-Gewicht Will man statt des \\(b\\)-Gewichtes das standardisierte \\(\\beta\\)-Gewicht angeben, muss in der Modellformel z-transformiert werden. Dafür könenn wir entweder alle Teile der formula scalen: (fitZ &lt;- lm(scale(hawie_wahr_log) ~ scale(hawie_iq), data = df_wide)) ## ## Call: ## lm(formula = scale(hawie_wahr_log) ~ scale(hawie_iq), data = df_wide) ## ## Coefficients: ## (Intercept) scale(hawie_iq) ## -4.971e-16 7.591e-01 Oder wir benutzen die Index-pipe %$% aus dem magrittr-Paket und ein zwischengeschaltetes mutate, um die Skalierung ein bisschen übersichtlicher zu gestalten: library(magrittr) fitZ &lt;- df_wide %&gt;% mutate(hawie_wahr_log = scale(hawie_wahr_log), hawie_iq = scale(hawie_iq)) %$% lm(hawie_wahr_log ~ hawie_iq) fitZ ## ## Call: ## lm(formula = hawie_wahr_log ~ hawie_iq) ## ## Coefficients: ## (Intercept) hawie_iq ## -4.971e-16 7.591e-01 df_wide %&gt;% mutate(hawie_wahr_log = scale(hawie_wahr_log), hawie_iq = scale(hawie_iq)) %&gt;% ggplot(aes(x = scale(hawie_iq), y = scale(hawie_wahr_log))) + geom_point() + geom_smooth(formula = y ~ x , method = &#39;lm&#39;,col =&#39;red&#39;,se = F) weitere Parameter lm() gibt eine Liste zurück, die ein deskriptives Modell der Daten darstellt. R bietet weitere Funktionen um einzelne Parameter dieses Outputs auszulesen. Zum Beispiel: residuals() zum Anzeigen der Residuen, coef() zur Ausgabe der geschätzten Modellparameter und fitted() für die vorhergesagten Werte. residuals(fitZ) ## 1 2 3 4 ## -0.132244120 0.553125355 -0.286404915 0.288579211 ## 5 6 7 8 ## 1.748985446 -0.244745860 -0.946823542 0.371897320 ## 9 10 11 12 ## 0.611492617 -0.075993249 -0.036450587 -0.357247601 ## 13 14 15 16 ## -0.034334195 -0.736411877 1.438547463 -0.469749341 ## 17 18 19 20 ## -0.146835935 0.682335303 -0.244745860 -0.905164488 ## 21 22 23 24 ## -1.634309409 -0.330180362 -0.200970413 -0.161427751 ## 25 26 27 28 ## 0.121942993 0.723994358 -0.721820061 0.682335303 ## 29 30 31 32 ## -0.146835935 0.136534808 0.721877965 -0.088468673 ## 33 34 35 36 ## -0.455157526 0.430264583 -1.198894262 -0.655210160 ## 37 38 39 40 ## -0.203086806 -0.103060488 0.234444733 -0.273929492 ## 41 42 43 44 ## 0.119826600 0.401080952 -0.384314840 0.065692122 ## 45 46 47 48 ## 1.342753931 -1.046849860 0.626084433 0.009441252 ## 49 50 ## 0.482282669 0.428148191 ## attr(,&quot;scaled:center&quot;) ## [1] 9.96 ## attr(,&quot;scaled:scale&quot;) ## [1] 3.096805 Im broom-Paket gibt es außerdem die augment-Funktion, die uns den zum Fitten genutzten Datensatz mit einer Reihe von Zusatzinfos ausgibt. broom::augment(fitZ) ## # A tibble: 50 × 8 ## hawie_wahr_log[,1] hawie_iq[,1] .fitted .resid ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.310 -0.234 -0.178 -0.132 ## 2 1.95 1.84 1.40 0.553 ## 3 -0.633 -0.456 -0.347 -0.286 ## 4 0.336 0.0622 0.0473 0.289 ## 5 1.63 -0.160 -0.122 1.75 ## 6 -0.310 -0.0860 -0.0653 -0.245 ## 7 -0.956 -0.0119 -0.00900 -0.947 ## 8 0.982 0.803 0.610 0.372 ## 9 0.659 0.0622 0.0473 0.611 ## 10 -0.310 -0.308 -0.234 -0.0760 ## .hat .sigma .cooksd .std.resid ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0211 0.664 0.000445 -0.203 ## 2 0.0892 0.659 0.0380 0.881 ## 3 0.0243 0.663 0.00241 -0.441 ## 4 0.0201 0.663 0.00201 0.443 ## 5 0.0205 0.613 0.0756 2.69 ## 6 0.0202 0.664 0.00145 -0.376 ## 7 0.0200 0.650 0.0216 -1.45 ## 8 0.0332 0.662 0.00567 0.575 ## 9 0.0201 0.659 0.00904 0.939 ## 10 0.0219 0.665 0.000153 -0.117 ## # … with 40 more rows fitZ %&gt;% broom::augment() %&gt;% ggplot(aes(x = hawie_iq, y = hawie_wahr_log))+ geom_linerange(aes(ymin = .fitted, ymax = hawie_wahr_log), col = &#39;orange&#39;) + geom_point() + geom_smooth(formula = y ~ x , method = &#39;lm&#39;,col =&#39;red&#39;,se = F) Regressionsanalyse Test-Hintergrund Unter Voraussetzungen von Varianzhomogenität und Normalverteilung der \\(Y\\)-Werte für jeden möglichen Wert von \\(X\\) können Regressionskoeffizienten ähnlich wie Korrelationskoeffizienten auf Unterschiedlichkeit von 0 getestet werden. Dazu wird genutzt, dass der Term \\(t = {{b}\\over{{s_{Y \\cdot X}}\\over {s_X \\sqrt{N-1}}}}\\) \\(t_{N-1}\\)-verteilt ist, wenn das tatsächliche \\(b^*\\) nicht unterschiedlich von 0 ist und für jeden Wert von \\(X\\) \\(Y\\) normalverteilt ist mit \\(\\mu = b^*X + a^*\\) und einer Varianz \\(\\sigma^2\\). Die Nullhypothese ist also \\(\\text{H}_0: b^* = 0\\). Test in R Um zusätzliche Informationen (insbesondere inferenzstatistische Kennwerte) eine mit lm() erstellten Regressions-Modells zu erhalten, kann einfach summary() verwendet werden. fitZ %&gt;% summary() ## ## Call: ## lm(formula = hawie_wahr_log ~ hawie_iq) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.63431 -0.31924 -0.08223 0.42138 1.74899 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.971e-16 9.302e-02 0.000 1 ## hawie_iq 7.591e-01 9.397e-02 8.078 1.68e-10 ## ## (Intercept) ## hawie_iq *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6578 on 48 degrees of freedom ## Multiple R-squared: 0.5762, Adjusted R-squared: 0.5674 ## F-statistic: 65.26 on 1 and 48 DF, p-value: 1.678e-10 Außerdem gibt es auch hier einen hübschen broom-Output, für den wir mit conf.int angeben können, Konfidenzintervalle für die Regressionsgewichte ausgeben lassen zu wollen: fitZ %&gt;% broom::tidy(conf.int = T) ## # A tibble: 2 × 7 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -4.97e-16 0.0930 -5.34e-15 1.00e+ 0 ## 2 hawie_iq 7.59e- 1 0.0940 8.08e+ 0 1.68e-10 ## conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.187 0.187 ## 2 0.570 0.948 Aufgabe fitZ %&gt;% broom::tidy(conf.int = T) ## # A tibble: 2 × 7 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -4.97e-16 0.0930 -5.34e-15 1.00e+ 0 ## 2 hawie_iq 7.59e- 1 0.0940 8.08e+ 0 1.68e-10 ## conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.187 0.187 ## 2 0.570 0.948 Wie lässt sich das Ergebnis interpretieren? A: Höhere IQ-Werte hängen mit höheren Logik-Leistungswerten zusammen. B: Es gibt keine Korrelation zwischen IQ-Werten und Logik-Leistung. C: Mit jedem Anstieg des IQ um eine Streuungs-Einheit, steigt die Vorhersage um 0.7590665 Streuungs-Einheiten. D: Man kann wegen der unzureichenden Berücksichtigung nicht-linearer Zusammenhänge keine Aussage treffen. Lösung C könnte man so sagen. "],["lineare-zusammenhänge-ii.html", "Lineare Zusammenhänge II Multiple Lineare Regression Regressionsdiagnostik", " Lineare Zusammenhänge II Datensatz Wir benutzen wieder den Datensatz df_wide aus der letzten Woche. Hier nochmal eine Übersicht: Variable Inhalt group Treatment-Gruppe pre_skill motorischer Skill vor dem Treatment post_skill motorischer Skill nach dem Treatment hawie_iq Intelligenz-Quotient aus HAWIE hawie_wahr_log Skalenwert wahrnehmungsgebundenes logisches Denken aus HAWIE Multiple Lineare Regression Verfahren Bei der multiplen linearen Regression dienen mehrere quantitative oder dichotome Variablen \\(X_j\\) als Prädiktoren zur Vorhersage des quantitativen Kriteriums \\(Y\\). Die Vorhersagegleichung hat hier die Form \\(\\hat{Y} = a + b_1 X_1 + ... + b_j X_ j + ... + b_p X_p\\) , wobei die Koeffizienten \\(a\\) und \\(b_j\\) auf Basis der empirischen Daten zu ermitteln sind. Als R-formula sieht das wie folgt aus: \\[\\text{Kriterium} \\sim \\text{Prädiktor}_1 + \\dots + \\text{Prädiktor}_p\\] Man versucht also, eine stetige Variable durch eine Linearkombination mehrerer Variablen vorherzusagen. Dabei ist aber meistens eher das Ausmaß des Zusammenhangs als die tatsächliche Vorhersage für neue Werte interessant. Deskriptive Modellanpassung und Regressionsanalyse Regression vom motorischen Skill nach dem Training auf IQ und wahrnehmungsgebundenes logisches Denken: fit_post_il &lt;- df_wide %&gt;% lm(post_skill~hawie_iq+hawie_wahr_log, data=.) fit_post_il ## ## Call: ## lm(formula = post_skill ~ hawie_iq + hawie_wahr_log, data = .) ## ## Coefficients: ## (Intercept) hawie_iq hawie_wahr_log ## 5.75745 -0.05013 0.42397 Für standardisierte Gewichte wie vorher: library(magrittr) fit_post_il_z &lt;- df_wide %&gt;% mutate(across(where(is.numeric), ~scale(.))) %$% lm(post_skill~hawie_iq+hawie_wahr_log) Darstellung Mit der Funktion scatter3d() aus dem Paket car lassen sich die Daten dann dreidimensional mit Residuen plotten. library(car) scatter3d(post_skill~hawie_iq+hawie_wahr_log, data=df_wide) Ein bisschen komplizierter geht das auch mit dem Paket plotly. Dafür muss zuerst ein Datensatz erstellt werden, der den gesamten Raum der im darzustellenden Datensatz vorliegenden Prädiktoren abdeckt: dummy &lt;- df_wide %&gt;% select(hawie_iq, hawie_wahr_log) %&gt;% expand.grid() Für alle diese Kombinationen müssen wir jetzt mit unserem Modell eine Vorhersage treffen: dummy$post_skill &lt;- predict(fit_post_il, dummy) Und diesen Datensatz können wir jetzt mit plotly darstellen: library(plotly) plot_ly(data = df_wide, x = ~hawie_iq, y = ~hawie_wahr_log, z = ~post_skill, type = &#39;scatter3d&#39;, mode = &#39;markers&#39;) %&gt;% add_mesh(data = dummy) Regressionsdiagnostik Regressionsdiagnostik Regressionsgleichungen sind sehr anfällig für verschiedene Klassen von Ausreißern. Ein Extremwert kann unter bestimmten Bedingungen die errechneten Koeffizienten extrem beeinflussen und verzerren. Es gibt drei Klassen von diagnostischen Werten, die unterschiedliche Aspekte möglicher Verfälschung beleuchten. Abstand (mögliche Ausreißer im Wertebereich des Kriteriums) Hebelwirkung (mögliche Ausreißer im Wertebereich der Prädiktoren) Einfluss (Kombination von Abstand und Hebelwirkung) http://omaymas.github.io/InfluenceAnalysis/ gibt es eine Shiny-App, mit der man daran rumspielen kann. Abstand Die Plausibilität des Abstandes lässt sich am Besten mit Hilfe der Residuen der Regression überprüfen. Dafür benutzen wir hier eine grafische Darstellung, um einen Überblick über deren Verteilung zu erlangen. df_wide %&gt;% mutate(Residuen = residuals(fit_post_il)) %&gt;% ggplot(aes(y = Residuen, x = &#39;&#39;) ) + geom_violin(color = &#39;grey&#39;, fill = &#39;lightgrey&#39;, alpha = .5) + geom_boxplot(width = .5) + coord_flip() Hebelwirkung Hebelwirkung ist das Ausmaß, in dem ein Prädiktor-Wert ungewöhnlich in Bezug auf die restlichen Prädiktorwerte ist. Das für Aussagen darüber genutzte Maß sind die Hebelwerte. Bei der einfachen Regression sind Hebelwerte die Abweichung der einzelnen Prädiktorwerte von deren Mittelwert. Bei der multiplen Regression ist das nicht ganz so einfach. In beiden Fällen bewegt sich der Hebelwert aber zwischen \\(\\frac{1}{N}\\) und 1. Diese Hebelwerte werden mit hatvalues() berechnet. Faustregel: Bei \\(p\\) Einflussgrößen (Prädiktoren) und \\(N\\) Beobachtungen sind Fälle mit Hebelwerten von größer als \\(3 \\cdot \\frac{p+1}N\\) problematisch. h &lt;- hatvalues(fit_post_il) Mit einem Spikeplot lassen sich diese dann veranschaulichen. tibble(hats = h, ID = 1:50) %&gt;% ggplot(aes(x= ID, ymax = h, ymin= 0)) + geom_linerange() + geom_hline(yintercept = (3*(2+1)/50),col=&#39;red&#39;) + scale_x_continuous(breaks = seq(0,50,5)) + labs(y = &#39;hat-values&#39;) Einfluss Unter Einfluss oder influence versteht man den Einfluss eines einzelnen Datenpunktes auf die gesamte Vorhersage. Er stellt also eine Kombination der vorher genannten Parameter dar. Am besten lässt sich dieser mit folgender Funktion grafisch überprüfen: plot(fit_post_il, which = 5) Ganz hübsche Alternativen bieten auch die Funktion influencePlot() aus dem car-Paket: influencePlot(fit_post_il) Tab. 3: StudResHatCookD 0.2290.1680.00361 2.17&nbsp;0.0950.152&nbsp;&nbsp; -1.65&nbsp;0.1770.187&nbsp;&nbsp; 1.84&nbsp;0.1130.137&nbsp;&nbsp; Außerdem können wir uns auch hier das broom-Paket benutzen, diesmal um uns alle diagnostischen Werte in einem praktischen Datensatz ausgeben zu lassen: fit_post_il %&gt;% broom::augment() Tab. 4: post_skillhawie_iqhawie_wahr_log.fitted.resid.hat.sigma.cooksd.std.resid 29394.91-2.91&nbsp;&nbsp;&nbsp;0.022&nbsp;2.810.0082&nbsp;&nbsp;-1.05&nbsp;&nbsp; 9121166.482.52&nbsp;&nbsp;&nbsp;0.104&nbsp;2.820.0347&nbsp;&nbsp;0.948&nbsp; 89084.643.36&nbsp;&nbsp;&nbsp;0.02822.8&nbsp;0.0142&nbsp;&nbsp;1.21&nbsp;&nbsp; 197115.56-4.56&nbsp;&nbsp;&nbsp;0.02412.760.0221&nbsp;&nbsp;-1.64&nbsp;&nbsp; 894157.410.595&nbsp;&nbsp;0.168&nbsp;2.840.00361&nbsp;0.232&nbsp; 49594.81-0.811&nbsp;&nbsp;0.023&nbsp;2.840.000669-0.292&nbsp; 59673.911.09&nbsp;&nbsp;&nbsp;0.06322.840.00358&nbsp;0.399&nbsp; 3107135.91-2.91&nbsp;&nbsp;&nbsp;0.03982.810.0154&nbsp;&nbsp;-1.05&nbsp;&nbsp; 1097125.984.02&nbsp;&nbsp;&nbsp;0.03812.780.028&nbsp;&nbsp;&nbsp;1.46&nbsp;&nbsp; 89294.963.04&nbsp;&nbsp;&nbsp;0.02222.810.00904&nbsp;1.09&nbsp;&nbsp; 6120145.680.322&nbsp;&nbsp;0.08382.840.0004360.12&nbsp;&nbsp; 49794.71-0.711&nbsp;&nbsp;0.02622.840.000589-0.256&nbsp; 697105.130.865&nbsp;&nbsp;0.02012.840.0006610.311&nbsp; 59884.240.763&nbsp;&nbsp;0.04652.840.00125&nbsp;0.278&nbsp; 8111177.4&nbsp;0.599&nbsp;&nbsp;0.144&nbsp;2.840.00298&nbsp;0.23&nbsp;&nbsp; 29994.61-2.61&nbsp;&nbsp;&nbsp;0.03152.820.00965&nbsp;-0.943&nbsp; 899105.032.97&nbsp;&nbsp;&nbsp;0.02192.810.0085&nbsp;&nbsp;1.07&nbsp;&nbsp; 890115.912.09&nbsp;&nbsp;&nbsp;0.04672.830.00945&nbsp;0.761&nbsp; 19594.81-3.81&nbsp;&nbsp;&nbsp;0.023&nbsp;2.790.0148&nbsp;&nbsp;-1.37&nbsp;&nbsp; 110184.09-3.09&nbsp;&nbsp;&nbsp;0.06212.8&nbsp;0.0283&nbsp;&nbsp;-1.13&nbsp;&nbsp; 29142.89-0.892&nbsp;&nbsp;0.152&nbsp;2.840.00706&nbsp;-0.344&nbsp; 5108115.01-0.007430.041&nbsp;2.841.03e-07-0.0027 47764.44-0.442&nbsp;&nbsp;0.06312.840.00059&nbsp;-0.162&nbsp; 4105115.16-1.16&nbsp;&nbsp;&nbsp;0.03&nbsp;&nbsp;2.840.0018&nbsp;&nbsp;-0.418&nbsp; 87774.873.13&nbsp;&nbsp;&nbsp;0.06192.8&nbsp;0.0291&nbsp;&nbsp;1.15&nbsp;&nbsp; 595126.08-1.08&nbsp;&nbsp;&nbsp;0.04542.840.00246&nbsp;-0.394&nbsp; 79274.112.89&nbsp;&nbsp;&nbsp;0.047&nbsp;2.810.0182&nbsp;&nbsp;1.05&nbsp;&nbsp; 890115.912.09&nbsp;&nbsp;&nbsp;0.04672.830.00945&nbsp;0.761&nbsp; 899105.032.97&nbsp;&nbsp;&nbsp;0.02192.810.0085&nbsp;&nbsp;1.07&nbsp;&nbsp; 17164.74-3.74&nbsp;&nbsp;&nbsp;0.09182.780.0657&nbsp;&nbsp;-1.4&nbsp;&nbsp;&nbsp; 5118166.63-1.63&nbsp;&nbsp;&nbsp;0.09862.830.0135&nbsp;&nbsp;-0.609&nbsp; 57564.540.458&nbsp;&nbsp;0.07062.840.0007230.169&nbsp; 49384.49-0.487&nbsp;&nbsp;0.03112.840.000331-0.176&nbsp; 108395.414.59&nbsp;&nbsp;&nbsp;0.04832.760.0473&nbsp;&nbsp;1.67&nbsp;&nbsp; 98953.425.58&nbsp;&nbsp;&nbsp;0.095&nbsp;2.710.152&nbsp;&nbsp;&nbsp;2.09&nbsp;&nbsp; 1131145.13-4.13&nbsp;&nbsp;&nbsp;0.177&nbsp;2.760.187&nbsp;&nbsp;&nbsp;-1.62&nbsp;&nbsp; 6100104.981.02&nbsp;&nbsp;&nbsp;0.02362.840.00108&nbsp;0.365&nbsp; 58174.660.335&nbsp;&nbsp;0.04632.840.0002410.122&nbsp; 27574.97-2.97&nbsp;&nbsp;&nbsp;0.07282.810.0314&nbsp;&nbsp;-1.09&nbsp;&nbsp; 1107115.06-4.06&nbsp;&nbsp;&nbsp;0.03682.780.0275&nbsp;&nbsp;-1.47&nbsp;&nbsp; 3100115.41-2.41&nbsp;&nbsp;&nbsp;0.02232.820.00571&nbsp;-0.866&nbsp; 695115.660.341&nbsp;&nbsp;0.02792.840.0001450.123&nbsp; 38674.41-1.41&nbsp;&nbsp;&nbsp;0.03872.840.00353&nbsp;-0.513&nbsp; 17874.82-3.82&nbsp;&nbsp;&nbsp;0.05722.780.0394&nbsp;&nbsp;-1.4&nbsp;&nbsp;&nbsp; 684126.63-0.634&nbsp;&nbsp;0.123&nbsp;2.840.00272&nbsp;-0.241&nbsp; 9115104.234.77&nbsp;&nbsp;&nbsp;0.113&nbsp;2.740.137&nbsp;&nbsp;&nbsp;1.8&nbsp;&nbsp;&nbsp; 991115.863.14&nbsp;&nbsp;&nbsp;0.04192.8&nbsp;0.0189&nbsp;&nbsp;1.14&nbsp;&nbsp; 37974.77-1.77&nbsp;&nbsp;&nbsp;0.053&nbsp;2.830.00776&nbsp;-0.645&nbsp; 10128176.553.45&nbsp;&nbsp;&nbsp;0.145&nbsp;2.790.0994&nbsp;&nbsp;1.33&nbsp;&nbsp; 1106135.96-4.96&nbsp;&nbsp;&nbsp;0.03972.740.0445&nbsp;&nbsp;-1.8&nbsp;&nbsp;&nbsp; Test der Regressionskoeffizienten Auch bei der multiplen linearen Regression lassen sich die Regressionskoeffizienten der einzelnen Prädiktoren jeweils auf die Nullhypothese testen, dass die jeweiligen “wahren Koeffizienten” \\({b_i}^*\\) gleich 0 sind. Die Schätzung der Streuung der Koeffizienten ist ein bisschen komplizierter als im einfachen Fall, deswegen sei hier nur erwähnt, dass es geht. Die für jeden Prädiktoren gebildete Teststatistik \\(t = \\frac{b_i}{s_{b_{i}}}\\) ist dann bei Gültigkeit der Nullhypothese (\\(H_0:{b_i}^* = 0\\)) \\(t_{N-p-1}\\)-verteilt, wobei \\(p\\) die Gesamtzahl der Prädiktoren und \\(N\\) die Gesamtzahl der Beobachtungen ist. Der Test der Parameter auf Signifikanz läuft wie im einfachen Fall mit der Funktion summary() summary(fit_post_il_z) ## ## Call: ## lm(formula = post_skill ~ hawie_iq + hawie_wahr_log) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.70753 -0.77446 0.05423 0.83241 1.92405 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.812e-17 1.371e-01 0.000 1.0000 ## hawie_iq -2.331e-01 2.127e-01 -1.096 0.2787 ## hawie_wahr_log 4.524e-01 2.127e-01 2.127 0.0387 ## ## (Intercept) ## hawie_iq ## hawie_wahr_log * ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9692 on 47 degrees of freedom ## Multiple R-squared: 0.09891, Adjusted R-squared: 0.06057 ## F-statistic: 2.58 on 2 and 47 DF, p-value: 0.0865 Oder auch wieder mit broom::tidy(): broom::tidy(fit_post_il_z) Tab. 5: termestimatestd.errorstatisticp.value (Intercept)6.81e-170.1374.97e-161&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hawie_iq-0.233&nbsp;&nbsp;&nbsp;0.213-1.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.279&nbsp; hawie_wahr_log0.452&nbsp;&nbsp;&nbsp;0.2132.13&nbsp;&nbsp;&nbsp;&nbsp;0.0387 Aufgabe Tab. 6: termestimatestd.errorstatisticp.value (Intercept)6.81e-170.1374.97e-161&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hawie_iq-0.233&nbsp;&nbsp;&nbsp;0.213-1.1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.279&nbsp; hawie_wahr_log0.452&nbsp;&nbsp;&nbsp;0.2132.13&nbsp;&nbsp;&nbsp;&nbsp;0.0387 Wie lässt sich das Ergebnis interpretieren? Der IQ liefert einen signifikanten Beitrag zur Vorhersage des motorischen Skills nach dem Training. Es gibt keine Korrelation zwischen IQ-Werten und dem motorischen Skill nach dem Training. Größere Werte auf der Skala für wahrnehmungsgebundenes logisches Denken bewirken eine signifikante Steigerung des motorischen Skills nach dem Training. Die Werte der Vorhersage steigen mit denen des wahrnehmungsgebundenen logischen Denkens und die Werte für wahrnehmungsgebundenes logisches Denken leisten einen signifikanten Beitrag zur Vorhersage des motorischen Skills nach der Intervention. Antwort Viertens kann man so sagen. Gegen 3 spricht die Kausalinterpretation, 1 ist verkehrt (keine Signifikanz) und über 2 können wir mit dem Ergebnis des Gesamtmodells direkt keine Aussage treffen. Test der Signifikanz von \\(\\text{R}^2\\) Man kann sich die Frage stellen, ob das Modell mit den gewählten Prädiktoren insgesamt das Kriterium gut vorhersagt. Das lässt sich am einfachsten bewerkstelligen, indem man den Determinationskoeffizienten \\(R^2\\) auf die \\(\\text{H}_0\\) testet, dass der ‘wahre’ Koeffizient \\(R^*\\) der Population gleich 0 ist. (“\\(\\text{H}_0: R^* = 0\\)”) Getestet wird diese Hypothese mit der Teststatistik \\(F = \\frac{(N-p-1)R^2}{p(1-R^2)}\\), die \\(F_{p,N-p-1}\\)-verteilt ist. Dabei ist \\(N\\) wieder die Anzahl der Beobachtungen und \\(p\\) die Anzahl der Prädiktoren. Um diesen Test durchzuführen können wir entweder auf den unteren Teil des summary-Outputs für ein Regressionsmodell gucken: ## Residual standard error: 0.9692 on 47 degrees of freedom ## Multiple R squared: 0.0989 , Adjusted R-squared: 0.0606 ## F-statistic: 2.58 on 2 and 47 DF, p-value: 0.0865 Oder die broom::glance()-Funktion nutzen: fit_post_il_z %&gt;% broom::glance() Tab. 7: r.squaredadj.r.squaredsigmastatisticp.valuedflogLikAICBICdeviancedf.residualnobs 0.09890.06060.9692.580.08652-67.814415144.24750 Aufgabe Tab. 8: r.squaredadj.r.squaredsigmastatisticp.valuedflogLikAICBICdeviancedf.residualnobs 0.09890.06060.9692.580.08652-67.814415144.24750 Wie lässt sich das Ergebnis interpretieren? Unser Modell ist ein sehr gutes Modell, es klärt einen signifikanten Teil der Varianz auf. Unser Modell sagt den motorischen Skill nicht gut voraus. Unser Modell klärt \\(\\sim\\) 10% der Varianz auf. Antwort 2 und 3 lassen sich so sagen. Prüfung der Voraussetzungen Für die Tests auf Signifikanz in der konventionellen Regressionsanalyse gelten die Voraussetzungen der Varianzhomogenität und der Normalverteiltheit der Residuen für jede einzelne Prädiktor-Kombination. Wir setzen also wieder voraus, dass \\(Y\\) für jede Kombination der \\(X_p\\) normalverteilt ist mit \\(\\mu = b_1^*X_1 + \\dots + b_p^*X_p + a^*\\) und einer Varianz \\(\\sigma^2\\). Zusätzlich zu den Voraussetzungen ist für die multiple Regression das Ausmaß der Multikollinearität der Prädiktoren relevant. Grafische Prüfverfahren Um die Anforderungen an die Messfehler der Regression heuristisch zu überprüfen, lassen sich verschiedene grafische Darstellungen heranziehen. Dazu benutzt man am besten eine standardisierte Form der Residuen. Zwei davon haben sich durchgesetzt, zum einen die studentischen \\[E_{stud}=\\frac{\\frac{E}{s}}{(1-\\frac{1}{N} + h)^{\\frac{1}{2}}}\\] und zum anderen die standardisierten \\[E_{stan}=\\frac{Y - \\hat{Y}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n e_i^2}}\\] Residuen. Die Residuen lassen sich dafür mit rstudent() oder rstandard() berechnen. Hier wird die broom::augment()-Funktion praktisch, da wir in dem ausgegebenen Datensatz die Residuen und die vorhergesagten Werte praktisch aufbereitet haben Die Verteilungseigenschaften können wir dann grafisch-heuristisch mit einem qq-Plot überprüfen: fit_post_il %&gt;% broom::augment() %&gt;% ggplot(aes(sample = .std.resid)) + geom_qq() + geom_qq_line() Je weiter die Punkte von der eingezeichneten Gerade abweichen, desto weniger können wir von einer Normalverteiltheit der Residuen ausgehen. Wie groß “noch akzeptable” Abweichung ist, ist ein Stück weit Gefühlssache. Hier findet man eine kleine shiny-App, mit der an qq-plots rumgespielt werden kann: Die Voraussetzung der Varianzhomogenität lässt sich auch so verstehen, dass wir für Y für jede Werte-Kombination unserer Prädiktoren eine gleich große Varianz voraussetzen. Über den Verlauf unserer vorhergesagten Werte sollten wir also um den 0-Punkt ungefähr gleich (breit) streuende Residuen beobachten können. Die genaue Form ist dabei aber natürlich abhängig von der Verteilung der Prädiktoren. Wenn wir diesem Plot jetzt noch einen lokalen Schätzer des Mittelwerts hinzufügen, haben wir einen so genannten Spread-Level-Plot: fit_post_il %&gt;% broom::augment() %&gt;% ggplot(aes(x = .fitted, y = .std.resid)) + geom_point() + geom_hline(yintercept = 0, color = &#39;red&#39;) + geom_smooth(formula = &#39;y~x&#39;,se = F, method = &#39;loess&#39;) In unserem Fall haben wir scheinbar ein paar Ausreißer (wie vorher auch schon gesehen), sonst ist die Punktwolke aber unproblematisch. Die Abweichung der loess-Regression sieht oft dramatischer aus als es faktisch ist, vor allem bei wenigen Beobachtungen wie bei uns. Man muss immer im Hinterkopf behalten, dass das genaue Bild stark von der Verteilung der Prädiktoren abhängt. So ist keins der folgenden Muster wirklich eine typische Punktwolke, trotzdem sind die Voraussetzungen überall gegeben: Wirklich Grund zur Sorge sollten uns Bilder wie die folgenden geben: Inferenstatistischer Test der Voraussetzungen Inferenz-statistisch lässt sich die Normalverteilung der Residuen zum Beispiel mit dem Kolmogorov-Smirnov-Test überprüfen: ks.test(x = rstudent(fit_post_il), y = &#39;pnorm&#39;) ## ## One-sample Kolmogorov-Smirnov test ## ## data: rstudent(fit_post_il) ## D = 0.093678, p-value = 0.7726 ## alternative hypothesis: two-sided Aufgabe ## ## One-sample Kolmogorov-Smirnov test ## ## data: rstudent(fit_post_il) ## D = 0.093678, p-value = 0.7726 ## alternative hypothesis: two-sided Wie lässt sich das Ergebnis interpretieren? 1. \\(1-p\\) ist kleiner als 30%, deswegen können wir keine Normalverteilung annehmen. 2. p ist größer als 20%; da wir eine Normalverteiltheit nicht ausschließen können, nehmen wir diese Voraussetzung als gegeben an. 3. Wir können gar nichts sagen, der Test aller Residuen auf einmal ergibt keinen Sinn. Antwort Zweitens ist die übliche Interpretation, auch wenn dem Herrn Andres hier der Dampf aus den Ohren steigt. Multikollinearität Multikollinearität liegt dann vor, wenn sich die Werte eines Prädiktors gut aus einer Linearkombination der übrigen Prädiktoren vorhersagen lassen. Dies ist insbesondere dann der Fall, wenn Prädiktoren paarweise miteinander hoch korrelieren. Für die multiple Regression hat dies weniger stabile Schätzungen der Koeffizienten als unerwünschte Konsequenz. Das heißt für die Praxis, dass die Regressionsgewichte schwer interpretierbar werden, sobald die entsprechenden Prädiktoren zu stark korrelieren, da von Stichprobe zu Stichprobe starke Änderungen zu erwarten sind. Modelle mit Multikolliniaren Prädiktoren haben aber meistens relativ stabile Determinationskoeffizienten, wenn uns also so oder so nur das Gesamtmodell interessiert, ist Multikollinearität kein allzu großes Problem. Als kleines Beispiel sind hier Simulationsergebnisse von 10000 Regressionen mit jeweils korrelierten Prädiktoren: Die Regression wurden jeweils auf einem nach dem folgenden Schema simulierten Datensatz erstellt: tibble(y = rnorm(100), x1 = y / 2 * runif(100,.5, 1.5), x2 = x1 * runif(100,.5, 1.5)) Tab. 9: yx1x2 0.193&nbsp;&nbsp;0.143&nbsp;&nbsp;0.0999&nbsp; 1.02&nbsp;&nbsp;&nbsp;0.735&nbsp;&nbsp;0.816&nbsp;&nbsp; 0.813&nbsp;&nbsp;0.21&nbsp;&nbsp;&nbsp;0.186&nbsp;&nbsp; 0.954&nbsp;&nbsp;0.361&nbsp;&nbsp;0.218&nbsp;&nbsp; 2.08&nbsp;&nbsp;&nbsp;0.931&nbsp;&nbsp;1.01&nbsp;&nbsp;&nbsp; 1.61&nbsp;&nbsp;&nbsp;0.71&nbsp;&nbsp;&nbsp;1.02&nbsp;&nbsp;&nbsp; 0.19&nbsp;&nbsp;&nbsp;0.142&nbsp;&nbsp;0.152&nbsp;&nbsp; -0.999&nbsp;&nbsp;-0.336&nbsp;&nbsp;-0.251&nbsp;&nbsp; -0.615&nbsp;&nbsp;-0.203&nbsp;&nbsp;-0.142&nbsp;&nbsp; 3.32&nbsp;&nbsp;&nbsp;1.41&nbsp;&nbsp;&nbsp;1.54&nbsp;&nbsp;&nbsp; 0.922&nbsp;&nbsp;0.545&nbsp;&nbsp;0.431&nbsp;&nbsp; 1.23&nbsp;&nbsp;&nbsp;0.467&nbsp;&nbsp;0.32&nbsp;&nbsp;&nbsp; -0.216&nbsp;&nbsp;-0.142&nbsp;&nbsp;-0.115&nbsp;&nbsp; -1.34&nbsp;&nbsp;&nbsp;-0.401&nbsp;&nbsp;-0.443&nbsp;&nbsp; -0.304&nbsp;&nbsp;-0.153&nbsp;&nbsp;-0.163&nbsp;&nbsp; -0.0779&nbsp;-0.0267&nbsp;-0.0381&nbsp; -0.193&nbsp;&nbsp;-0.0836&nbsp;-0.0737&nbsp; 0.123&nbsp;&nbsp;0.0864&nbsp;0.0447&nbsp; -0.279&nbsp;&nbsp;-0.16&nbsp;&nbsp;&nbsp;-0.122&nbsp;&nbsp; 0.758&nbsp;&nbsp;0.306&nbsp;&nbsp;0.297&nbsp;&nbsp; 0.0606&nbsp;0.0302&nbsp;0.0388&nbsp; -0.165&nbsp;&nbsp;-0.0743&nbsp;-0.0917&nbsp; 0.0715&nbsp;0.0189&nbsp;0.0136&nbsp; 0.649&nbsp;&nbsp;0.22&nbsp;&nbsp;&nbsp;0.261&nbsp;&nbsp; 1.1&nbsp;&nbsp;&nbsp;&nbsp;0.359&nbsp;&nbsp;0.369&nbsp;&nbsp; -1.37&nbsp;&nbsp;&nbsp;-0.553&nbsp;&nbsp;-0.538&nbsp;&nbsp; -0.127&nbsp;&nbsp;-0.0864&nbsp;-0.0849&nbsp; 1.08&nbsp;&nbsp;&nbsp;0.804&nbsp;&nbsp;1.09&nbsp;&nbsp;&nbsp; -1.45&nbsp;&nbsp;&nbsp;-0.736&nbsp;&nbsp;-0.967&nbsp;&nbsp; -0.237&nbsp;&nbsp;-0.171&nbsp;&nbsp;-0.0951&nbsp; 1.24&nbsp;&nbsp;&nbsp;0.378&nbsp;&nbsp;0.563&nbsp;&nbsp; -0.186&nbsp;&nbsp;-0.13&nbsp;&nbsp;&nbsp;-0.188&nbsp;&nbsp; -0.276&nbsp;&nbsp;-0.191&nbsp;&nbsp;-0.126&nbsp;&nbsp; 0.0368&nbsp;0.0203&nbsp;0.0299&nbsp; -0.492&nbsp;&nbsp;-0.18&nbsp;&nbsp;&nbsp;-0.184&nbsp;&nbsp; -0.0356&nbsp;-0.0168&nbsp;-0.0115&nbsp; 0.711&nbsp;&nbsp;0.48&nbsp;&nbsp;&nbsp;0.312&nbsp;&nbsp; 0.331&nbsp;&nbsp;0.127&nbsp;&nbsp;0.174&nbsp;&nbsp; -1.12&nbsp;&nbsp;&nbsp;-0.42&nbsp;&nbsp;&nbsp;-0.591&nbsp;&nbsp; -2.56&nbsp;&nbsp;&nbsp;-0.83&nbsp;&nbsp;&nbsp;-0.818&nbsp;&nbsp; -0.658&nbsp;&nbsp;-0.286&nbsp;&nbsp;-0.164&nbsp;&nbsp; 0.891&nbsp;&nbsp;0.35&nbsp;&nbsp;&nbsp;0.278&nbsp;&nbsp; -0.291&nbsp;&nbsp;-0.182&nbsp;&nbsp;-0.222&nbsp;&nbsp; 1.6&nbsp;&nbsp;&nbsp;&nbsp;1.01&nbsp;&nbsp;&nbsp;1.23&nbsp;&nbsp;&nbsp; -1.21&nbsp;&nbsp;&nbsp;-0.859&nbsp;&nbsp;-0.831&nbsp;&nbsp; -0.00831-0.00615-0.00542 -0.43&nbsp;&nbsp;&nbsp;-0.172&nbsp;&nbsp;-0.25&nbsp;&nbsp;&nbsp; -0.8&nbsp;&nbsp;&nbsp;&nbsp;-0.318&nbsp;&nbsp;-0.343&nbsp;&nbsp; 0.401&nbsp;&nbsp;0.254&nbsp;&nbsp;0.225&nbsp;&nbsp; 0.318&nbsp;&nbsp;0.147&nbsp;&nbsp;0.172&nbsp;&nbsp; -1.07&nbsp;&nbsp;&nbsp;-0.538&nbsp;&nbsp;-0.742&nbsp;&nbsp; -0.0746&nbsp;-0.0544&nbsp;-0.0409&nbsp; 1.3&nbsp;&nbsp;&nbsp;&nbsp;0.326&nbsp;&nbsp;0.37&nbsp;&nbsp;&nbsp; 0.0425&nbsp;0.0114&nbsp;0.00614 -0.0178&nbsp;-0.00456-0.00668 -1.89&nbsp;&nbsp;&nbsp;-0.616&nbsp;&nbsp;-0.625&nbsp;&nbsp; -1.05&nbsp;&nbsp;&nbsp;-0.666&nbsp;&nbsp;-0.869&nbsp;&nbsp; 1.16&nbsp;&nbsp;&nbsp;0.613&nbsp;&nbsp;0.724&nbsp;&nbsp; -0.644&nbsp;&nbsp;-0.352&nbsp;&nbsp;-0.297&nbsp;&nbsp; -1.48&nbsp;&nbsp;&nbsp;-0.584&nbsp;&nbsp;-0.526&nbsp;&nbsp; 0.113&nbsp;&nbsp;0.0788&nbsp;0.064&nbsp;&nbsp; -0.888&nbsp;&nbsp;-0.265&nbsp;&nbsp;-0.278&nbsp;&nbsp; -0.854&nbsp;&nbsp;-0.509&nbsp;&nbsp;-0.583&nbsp;&nbsp; -0.921&nbsp;&nbsp;-0.294&nbsp;&nbsp;-0.348&nbsp;&nbsp; 1.66&nbsp;&nbsp;&nbsp;0.554&nbsp;&nbsp;0.603&nbsp;&nbsp; 0.139&nbsp;&nbsp;0.0635&nbsp;0.0656&nbsp; 0.843&nbsp;&nbsp;0.605&nbsp;&nbsp;0.895&nbsp;&nbsp; 0.163&nbsp;&nbsp;0.086&nbsp;&nbsp;0.0605&nbsp; -0.486&nbsp;&nbsp;-0.347&nbsp;&nbsp;-0.319&nbsp;&nbsp; -0.746&nbsp;&nbsp;-0.428&nbsp;&nbsp;-0.546&nbsp;&nbsp; -0.793&nbsp;&nbsp;-0.224&nbsp;&nbsp;-0.318&nbsp;&nbsp; 0.299&nbsp;&nbsp;0.0778&nbsp;0.0809&nbsp; -1.76&nbsp;&nbsp;&nbsp;-0.666&nbsp;&nbsp;-0.359&nbsp;&nbsp; 0.0866&nbsp;0.0589&nbsp;0.0726&nbsp; 0.13&nbsp;&nbsp;&nbsp;0.0451&nbsp;0.042&nbsp;&nbsp; -1.33&nbsp;&nbsp;&nbsp;-0.75&nbsp;&nbsp;&nbsp;-0.963&nbsp;&nbsp; -0.118&nbsp;&nbsp;-0.0341&nbsp;-0.0375&nbsp; -1.16&nbsp;&nbsp;&nbsp;-0.393&nbsp;&nbsp;-0.512&nbsp;&nbsp; 0.172&nbsp;&nbsp;0.0876&nbsp;0.113&nbsp;&nbsp; -0.952&nbsp;&nbsp;-0.496&nbsp;&nbsp;-0.403&nbsp;&nbsp; 0.145&nbsp;&nbsp;0.0364&nbsp;0.0343&nbsp; -0.45&nbsp;&nbsp;&nbsp;-0.169&nbsp;&nbsp;-0.14&nbsp;&nbsp;&nbsp; 0.159&nbsp;&nbsp;0.117&nbsp;&nbsp;0.0749&nbsp; -0.51&nbsp;&nbsp;&nbsp;-0.37&nbsp;&nbsp;&nbsp;-0.554&nbsp;&nbsp; -0.402&nbsp;&nbsp;-0.146&nbsp;&nbsp;-0.108&nbsp;&nbsp; 0.526&nbsp;&nbsp;0.218&nbsp;&nbsp;0.219&nbsp;&nbsp; -0.907&nbsp;&nbsp;-0.279&nbsp;&nbsp;-0.406&nbsp;&nbsp; -2.32&nbsp;&nbsp;&nbsp;-1.21&nbsp;&nbsp;&nbsp;-1.64&nbsp;&nbsp;&nbsp; -1.19&nbsp;&nbsp;&nbsp;-0.804&nbsp;&nbsp;-0.852&nbsp;&nbsp; 0.358&nbsp;&nbsp;0.0916&nbsp;0.0977&nbsp; 0.124&nbsp;&nbsp;0.087&nbsp;&nbsp;0.0518&nbsp; -0.53&nbsp;&nbsp;&nbsp;-0.176&nbsp;&nbsp;-0.151&nbsp;&nbsp; 1.28&nbsp;&nbsp;&nbsp;0.762&nbsp;&nbsp;0.535&nbsp;&nbsp; -0.442&nbsp;&nbsp;-0.236&nbsp;&nbsp;-0.247&nbsp;&nbsp; -0.00543-0.00289-0.00363 -0.719&nbsp;&nbsp;-0.253&nbsp;&nbsp;-0.321&nbsp;&nbsp; -0.525&nbsp;&nbsp;-0.317&nbsp;&nbsp;-0.305&nbsp;&nbsp; -0.956&nbsp;&nbsp;-0.542&nbsp;&nbsp;-0.752&nbsp;&nbsp; 0.646&nbsp;&nbsp;0.418&nbsp;&nbsp;0.258&nbsp;&nbsp; 0.397&nbsp;&nbsp;0.248&nbsp;&nbsp;0.159&nbsp;&nbsp; Insgesamt verteilen sich die Ergebnisse wie folgt: Gewisse Linearkombinationen der Koeffizienten sind deutlich stabiler als die Koeffizienten allein, wie man hier am Beispiel von \\(b_1 + b_2 + 2\\) gut sehen kann. Welche Linearkombination gerade besonders stabil sein könnte, kann man einfach an einer Punktwolke wie der folgenden ablesen: Paarweise lineare Zusammenhänge lassen sich anhand der Korrelationsmatrix der Prädiktoren prüfen. df_wide %&gt;% select(hawie_iq, hawie_wahr_log) %&gt;% cor() ## hawie_iq hawie_wahr_log ## hawie_iq 1.0000000 0.7590665 ## hawie_wahr_log 0.7590665 1.0000000 Faustregel: Korrelationen &gt; 0.8 weisen auf starke Kollinearität hin. Der Varianzinflationsfaktor \\(\\text{VIF}_j=\\frac{1}{1-R_j^2}\\) jedes Prädiktors \\(j\\) liefert eine weitere Möglichkeit zur Kollinearitätsdiagnostik. Er kann mit der Funktion vif() aus dem Paket car berechnet werden. library(car) vif(fit_post_il) ## hawie_iq hawie_wahr_log ## 2.359503 2.359503 Faustregel: VIF-Faktor &gt; 4 \\(\\rightarrow\\) starke Multikollinearität "],["gruppenunterschiede-i.html", "Gruppenunterschiede I t-Test Varianzanalyse - ein unabhängiger Faktor", " Gruppenunterschiede I Datensatz Wir verwenden den (leicht erweiterten), simulierten Datensatz aus den letzten Wochen: Variable Inhalt vp_nr VP-Nummer group Treatment-Gruppe sex Geschlecht hawie_iq Intelligenz-Quotient aus HAWIE hawie_wahr_log Skalenwert wahrnehmungsgebundenes logisches Denken aus HAWIE pre_skill motorischer Skill vor dem Treatment post_skill motorischer Skill nach dem Treatment t-Test Ein-Stichproben-Problem Beim Ein-Stichproben-t-Test wird der Erwartungswert einer Stichprobe gegen den bekannten Erwartungswert einer Population getestet.(\\(\\text{H}_0: \\mu = \\mu_0\\)) Der Test setzt eigentlich voraus, dass die abhängige Variable normalverteilt ist. Wenn die Stichprobe groß genug ist, um eine näherungsweise Normalverteilung des Mittelwertes zu gewährleisten, kann man die \\(t\\)-Statistik aber ohne viele Probleme mit der entsprechenden \\(t\\)-Verteilung (\\(t_{n-1}\\)) verglichen werden. t.test(x=&lt;Vektor&gt;, alternative=c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), mu=0) muH0 &lt;- 100 (tResults &lt;- t.test(df_wide$hawie_iq, alternative=&quot;two.sided&quot;, mu=muH0)) ## ## One Sample t-test ## ## data: df_wide$hawie_iq ## t = -4.132, df = 149, p-value = 5.976e-05 ## alternative hypothesis: true mean is not equal to 100 ## 95 percent confidence interval: ## 91.76135 97.09199 ## sample estimates: ## mean of x ## 94.42667 Zwei-Stichproben-Problem, abhängiger Fall Beim abhängigen Zwei-Stichproben-Fall wird im Prinzip derselbe Test durchgeführt, nur dass hier die Differenzwerte der Beobachtungen als Stichprobe genutzt werden. Da die Frage hier nicht ist, ob der Erwartungswert einer Population einem spezifischen Erwartungswert gleich ist, sondern ob sich die Erwartungswerte der jeweiligen Beobachtungen unterscheiden, ist die Nullhypothese hier: \\[\\text{H}_0: { \\mu}_D = { \\mu}_1 - { \\mu}_2= 0 \\] Die Teststatistik ist hier auch \\(t_{n-1}\\)-verteilt. Dabei steht das \\(n\\) hier aber für die Anzahl der Beobachtungs-Paare, nicht die Gesamtzahl der Beobachtungen. t.test(x=df_wide$pre_skill, y=df_wide$post_skill, alternative=&quot;two.sided&quot;, paired=TRUE) ## ## Paired t-test ## ## data: df_wide$pre_skill and df_wide$post_skill ## t = 0.9628, df = 149, p-value = 0.3372 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.2946581 0.8546581 ## sample estimates: ## mean of the differences ## 0.28 Geht auch für das long-Format und in Formel-Schreibweise: df_wide %&gt;% pivot_longer(cols = c(pre_skill,post_skill), names_to = &#39;time&#39;, values_to = &#39;skill&#39;) %&gt;% t.test(skill ~ time, data=., alternative = &#39;less&#39;, paired=T) ## ## Paired t-test ## ## data: skill by time ## t = -0.9628, df = 149, p-value = 0.1686 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf 0.2013441 ## sample estimates: ## mean of the differences ## -0.28 Und genau wie bei den Regressionsanalysen können wir auch hier wieder broom benutzen, um eine übersichtliche Tabelle zu erhalten: df_wide %$% t.test(x=pre_skill, y=post_skill, alternative=&quot;two.sided&quot;, paired=TRUE) %&gt;% broom::tidy() Tab. 10: estimatestatisticp.valueparameterconf.lowconf.highmethodalternative 0.280.9630.337149-0.2950.855Paired t-testtwo.sided Aufgabe ## [1] &quot;t = -0.9628, df = 149, p-value = 0.1686&quot; Wie interpretieren wir das Ergebnis? Die Skillwerte zwischen Pre- und Post-Zeitpunkt sind nicht unterschiedlich. Wir verwerfen die \\(\\text{H}_0\\) und nehmen an, dass die Skillwerte der Population sich zu den zwei Zeitpunkten unterschieden. Wir können die \\(H_0\\) nicht verwerfen. Weil wir unabhängige Testzeitpunkte haben, ist das der falsche Test, das Ergebnis ist also tendenziell zu liberal. Antwort 1 ist eine Interpretation der \\(H_0\\), 2 wäre richtig wäre der Test signifikant, 4 ist ein möchtegern-schlauer Distraktor. Richtig ist 3. Zwei-Stichproben-Problem, unabhängiger Fall Beim unabhängigen Stichproben t-Test wird getestet, ob die Erwartungswerte zweier Stichproben unterschiedlich sind (\\(H_0: {\\mu}_1 = {\\mu}_2\\)). Dabei wird vorausgesetzt, dass die abhängige Variable in beiden Gruppen normalverteilt ist und dieselbe Varianz hat. Gelten diese Voraussetzungen, ist die \\(t\\)-Statistik unter der \\(H_0\\) \\(t_{n_1 + n_2 - 2}\\)-verteilt. t.test(x=&lt;Vektor&gt;, y=&lt;Vektor&gt;, paired=FALSE, alternative=c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;)) Auch als Formelschreibweise: df_wide %&gt;% t.test(post_skill ~ sex, data=., alternative=&quot;less&quot;, paired=FALSE,var.equal=TRUE) ## ## Two Sample t-test ## ## data: post_skill by sex ## t = -1.7207, df = 148, p-value = 0.0437 ## alternative hypothesis: true difference in means between group f and group m is less than 0 ## 95 percent confidence interval: ## -Inf -0.02840063 ## sample estimates: ## mean in group f mean in group m ## 4.626667 5.373333 Ungleiche Varianzen (oder Stichprobengrößen) Im Falle einer Varianzinhomogenität ist die t-Statistik nicht mehr gut mit der \\(t_{n_1 + n_2 - 2}\\)-Verteilung zu vergleichen. Die Welch-Korrektur kann in diesem Fall dazu dienen, eine Anzahl von Freiheitsgraden zu schätzen, die besser zu der Verteilung unter der \\(H_0\\) der Teststatistik passt. Mit dem var.equal-Argument lässt sich logisch angeben, ob die Freiheitsgrade nach Welch geschätzt werden sollen. Dabei ist FALSE der Standard. df_wide %&gt;% t.test(post_skill ~ sex, data=., alternative=&quot;less&quot;, paired=FALSE,var.equal=F) ## ## Welch Two Sample t-test ## ## data: post_skill by sex ## t = -1.7207, df = 145.74, p-value = 0.04372 ## alternative hypothesis: true difference in means between group f and group m is less than 0 ## 95 percent confidence interval: ## -Inf -0.02833062 ## sample estimates: ## mean in group f mean in group m ## 4.626667 5.373333 Test von Voraussetzungen Obwohl t-Tests relativ robust gegen Verletzungen der Normalverteilungsvoraussetzung sind, kann man auf die Idee kommen, auf die Normalverteiltheit der AV zu testen Außerdem kann für den Zwei-Stichproben-Test überlegt werden, ob die Varianzhomogenität überprüft werden soll, um über das genutzte Verfahren zu entscheiden. Test auf Normalverteilung Der Kolmogorov-Smirnov-Test auf eine feste Verteilung vergleicht die kumulierten relativen Häufigkeiten von Daten einer stetigen Variable mit einer frei wählbaren Verteilungsfunktion – etwa der einer bestimmten Normalverteilung. Hier wird er aber nur der Vollständigkeit halber erwähnt, da er stark abhängig von der Stichprobengröße schnell zu suboptimalen Handlungen führen kann. ks.test(x=&lt;Vektor&gt; , y=&quot;&lt;Name der Verteilungsfunktion&gt;&quot;, alternative=c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;)) ks.test(df_wide$hawie_iq, &#39;pnorm&#39;, 100, 15, alternative=&#39;two.sided&#39;) ## ## One-sample Kolmogorov-Smirnov test ## ## data: df_wide$hawie_iq ## D = 0.19056, p-value = 3.715e-05 ## alternative hypothesis: two-sided Bei kleinem N kann der Shapiro-Wilk-Test auf unspezifische Normalverteilung bessere Ergebnisse liefern, bei großem N hat dieser aber dasselbe Problem wie der KS-Test, und wird eigentlich immer signifikant. shapiro.test(df_wide$hawie_iq) ## ## Shapiro-Wilk normality test ## ## data: df_wide$hawie_iq ## W = 0.99, p-value = 0.3654 Außerdem möglich sind grafisch-heuristische Testvarianten: Aufgabe Was ist daran falsch? Nichts, alle diese Verfahren können so angewendet werden Verfahren zum Testen von Verteilungsvoraussetzungen sind absurd! die Verfahren, die wir meistens benutzen, sind gegen alle Verletzungen der Voraussetzungen robust! Über alle Gruppen hinweg einen Test auf Normalverteilung durchzuführen ist nicht sinnvoll. Grafiken sind deutlich weniger aussagekräftig als inferenzstatistische Tests, die haben hier nichts zu suchen. Antwort ist falsch, die Av über die Zellen zu poolen, ergibt keinen Sinn. könnte man argumentieren, bei sehr extremen Verletzungen oder der Verletzung mehrerer Voraussetzungen gleichzeitig kann man aber in Schwierigkeiten kommen ist richtig, siehe 1. sehe ich nicht so. Grafisch-heuristische Tests haben ihren heuristischen Entscheidungsanteil wenigstens im Namen. Test auf Varianzhomogenität Varianzhomogenität lässt sich mit dem Levene-Test testen, einem Signifikanz-Test, der auf Abweichungen von der Varianzhomogenität in zwei oder mehr Populationen testet. Im Prinzip wird die absolute Abweichung jedes Wertes einer Gruppe vom Zentrum dieser Gruppe berechnet. Über die resultierenden Werte wird dann eine Varianzanalyse gerechnet. Dafür kann in R die Funktion leveneTest() aus dem car-Paket verwendet werden. Dabei testet der Standard nicht die von Levene ursprünglich aufgestellte Differenz zum Mittelwert, sondern die Version von Brown und Forsythe, bei der die Differenz zum Median gebildet wird. Levene-Test für einen Faktor library(car) df_wide %&gt;% leveneTest(hawie_iq ~ sex, data=.) Tab. 11: DfF valuePr(&gt;F) 10.3410.56 148&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Levene-Test für mehr als einen Faktor Mit Verknüpfung zweier Faktoren als Interaktion im Modellterm wird der Test als assoziierte einfaktorielle Varianzanalyse ausgeführt. df_wide %&gt;% leveneTest(hawie_iq ~ group * sex, data=.) Tab. 12: DfF valuePr(&gt;F) 50.4890.784 144&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Varianzanalyse - ein unabhängiger Faktor Ein unabhängiger Faktor Bei der einfaktoriellen, unabhängigen Varianzanalyse testen wir eine Variable in einer Reihe von Bedingungen darauf, ob die jeweiligen Erwartungswerte unterschiedlich sind. Bei Gültigkeit der \\(\\text{H}_0: {\\mu}_1 = {\\mu}_2 \\ldots = {\\mu}_J\\), sowie Unabhängigkeit, Normalverteiltheit und gleicher Varianz der AV in allen Bedingungen ist die Teststatistik \\(F_{J-1, N-J}\\)-verteilt. Für Varianzanalysen gibt es eine ganze Reihe von Paketen und Funktionen, base-R hat alleine drei, die man für diesen Zweck nutzen kann. Für die bessere Integration in den tidyverse-workflow werden wir hier aber die Funktionen aus dem ez-Paket nutzen. ezANOVA(data = &lt;data&gt;, dv = &lt;AV&gt;, wid = &lt;VP-Code&gt;, between = &lt;UV&gt;) Als Beispiel sollen die Werte der zentralen Leistungs-AV zum ersten Messzeitpunkt in der follow-up Phase zwischen den Gruppen verglichen werden. library(ez) anova_group &lt;- df_wide %&gt;% ezANOVA(dv = post_skill, wid = vp_nr, between = group) Ergebnis des Tests: anova_group ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ## 1 group 2 147 17.76712 1.227478e-07 * ## ges ## 1 0.1946717 ## ## $`Levene&#39;s Test for Homogeneity of Variance` ## DFn DFd SSn SSd F p p&lt;.05 ## 1 2 147 2.773333 326.8 0.6237454 0.5373457 Der Output besteht aus den Ergebnistabellen für zwei inferenzstatistische Tests. Zum einen ist da das Ergebnis eines Levene-Tests: anova_group$`Levene&#39;s Test for Homogeneity of Variance` Tab. 13: DFnDFdSSnSSdFpp&lt;.05 21472.773270.6240.537 Mit Zähler- und Nennerfreiheitsgraden den entsprechenden Quadratsummen dem F-Wert dem entsprechenden p-Wert und einer Signifikanzaussage Zum Anderen dem Ergebnis der eigentlichen Varianzanalyse: anova_group$`ANOVA` Tab. 14: EffectDFnDFdFpp&lt;.05ges group214717.81.23e-07*0.195 Mit einer Angabe des Effekts, für den die Ergebnisse berichtet werden Zähler- und Nennerfreiheitsgraden den entsprechenden Quadratsummen dem F-Wert dem entsprechenden p-Wert und einer Signifikanzaussage einem Schätzer für die Effektstärke \\(\\eta^2\\) Aufgabe Tab. 15: EffectDFnDFdFpp&lt;.05ges group214717.81.23e-07*0.195 Wie interpretieren wir das Ergebnis? Es gibt keinen Unterschied zwischen den Gruppen. Der Test ist signifikant, deswegen nehmen wir an, dass die Interventionen zu größerem Skill führen. Der Test ist nicht signifikant, deswegen können wir keine Aussage treffen. Mit dem mittleren Effekt können wir aber mit G*Power eine Stichprobengröße ausrechnen, die zu Signifikanz führen würde. Der Test ist signifikant, deswegen entscheiden wir uns die Alternativhypothese anzunehmen, dass die Interventionen und die Kontrollgruppe sich in ihren Skill-Werten unterscheiden. Antwort ist falsch, der Test ist signifikant ist falsch, da eine Richtung mit-formuliert ist und wir ungerichtet getestet haben ist falsch da zum Einen der Test signifikant ist und zum Anderen so Power nicht funktioniert ist richtig Grafisch Voraussetzungen prüfen Die Voraussetzungen der Varianzanalyse lassen sich zu folgender Aussage umformulieren: Die Fehler müssen unabhängige, normalverteilte Variablen mit Erwartungswert 0 und gleicher Streuung sein. Diese umformulierten Anforderungen lassen sich dann wieder wie bei der Regressionsanalyse grafisch-heuristisch untersuchen. Damit wir die bekannten Funktionen zur Standardisierung der Fehler nutzen können, müssen wir aber noch das return_aov-Argument auf TRUE setzen. aov-Modelle sind die base-R Version, Ergebnisse von Varianzanalysen in einem ähnlichen Format wie die von linearen Modellen abzuspeichern. anova_group &lt;- df_wide %&gt;% ezANOVA(dv = post_skill, wid = vp_nr, between = group, return_aov = T) anova_group$aov ## Call: ## aov(formula = formula(aov_formula), data = data) ## ## Terms: ## group Residuals ## Sum of Squares 207.52 858.48 ## Deg. of Freedom 2 147 ## ## Residual standard error: 2.416609 ## Estimated effects may be unbalanced Paarvergleiche mit t-Tests und \\(\\alpha\\)-Adjustierung Besteht nach einer ANOVA die Frage, bei welchen Paaren Erwartungswertunterschiede vorliegen, können t-Tests durchgeführt werden, um die Unterschiede jeweils zweier Gruppen auf Signifikanz zu prüfen. with(df_wide, pairwise.t.test(post_skill, group, p.adjust.method=&quot;bonferroni&quot;, paired=F, pool.sd=T)) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: post_skill and group ## ## Control Training ## Training 0.12089 - ## Training2 0.00062 8.1e-08 ## ## P value adjustment method: bonferroni Das p.adjust.method-Argument übernimmt eine relativ große Zahl von Korrekturen: \"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\", \"fdr\", \"none\" "],["gruppenunterschiede-ii.html", "Gruppenunterschiede II Varianzanalyse - ein abhängiger Faktor Varianzanalyse - zwei unabhängige Faktoren Varianzanalyse - zwei abhängige Faktoren Varianzanalyse - unabhängige und abhängige Faktoren Beliebige Linearkontraste", " Gruppenunterschiede II Datensatz Wir verwenden den (leicht erweiterten), simulierten Datensatz aus den letzten Wochen: Variable Inhalt vp_nr VP-Nummer group Treatment-Gruppe sex Geschlecht hawie_iq Intelligenz-Quotient aus HAWIE hawie_wahr_log Skalenwert wahrnehmungsgebundenes  logisches Denken aus HAWIE pre_skill motorischer Skill vor dem Treatment peri_skill motorischer Skill während des Treatments post_skill motorischer Skill nach dem Treatment substance Substanzgabe vor der jeweiligen Testung Varianzanalyse - ein abhängiger Faktor ein abhängiger Faktor Die einfaktorielle Varianzanalyse mit abhängigen Gruppen dient dazu, Mittelwerte zu vergleichen, die aus einer Reihe von Beobachtungen stammen, bei denen statistische Abhängigkeit angenommen werden muss. Das ist zum Beispiel bei einem Design mit wiederholten Messungen in jeweils ein und derselben Versuchsperson der Fall. Wenn die Variablen normalverteilt, die wiederholten Messungen untereinander (zwischen den Versuchspersonen) unabhängig sind und Sphärizität gegeben ist, ist die Teststatistik im Falle der Gültigkeit der \\(\\text{H}_0: {\\mu}_1 = {\\mu}_2 \\ldots = {\\mu}_k\\) \\(F_{k-1,(k-1)(N-1)}\\)-verteilt. Für die Durchführung der Varianzanalyse mit abhängigen Gruppen mit ezANOVA() muss der Datensatz im LONG-Format vorliegen (zumindest muss die AV über alle Messzeitpunkte in einer Variable abgelegt sein) Ein Blockbildungsfaktor, also eine Kennzeichnung des Messzeitpunktes vorliegen wie vorher auch eine Variable, die die Versuchsperson codiert, vorliegen. Im Folgenden wird der Blockbildungsfaktor genannt. Der aus dem unabhängigen Fall bekannte Funktionsaufruf muss nur so geändert werden, dass statt des vorher mit dem between-Argument angegebenen unabhängigen Faktors mit dem within-Argument der abhängige oder Blockbildungsfaktor angegeben wird. ezANOVA(data = &lt;data&gt;, dv = &lt;AV&gt;, wid = &lt;VP-Code&gt;, within = &lt;Block&gt;) In unserem Beispiel-Datensatz sieht das so aus: library(ez) anova_time &lt;- df_wide %&gt;% pivot_longer(cols = contains(&#39;skill&#39;), names_to = &#39;time&#39;, values_to = &#39;skill&#39;) %&gt;% ezANOVA(dv = skill, wid = vp_nr, within = time) Das Ganze führt dann zu dem folgenden Ergebnis: anova_time ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 2 time 2 398 4.329244 0.01380361 * 0.01245873 ## ## $`Mauchly&#39;s Test for Sphericity` ## Effect W p p&lt;.05 ## 2 time 0.8678891 8.090194e-07 * ## ## $`Sphericity Corrections` ## Effect GGe p[GG] p[GG]&lt;.05 HFe ## 2 time 0.8833057 0.01757486 * 0.8906258 ## p[HF] p[HF]&lt;.05 ## 2 0.0173103 * Der Output besteht aus den Ergebnistabellen für zwei inferenzstatistische Tests und einer Ergebnistabelle für die Testergebnisse nach Sphärizitäts-Korrektur. Als Erstes ist da das Ergebnis eines Mauchly-Tests (ein Test auf Sphärizität): anova_time$`Mauchly&#39;s Test for Sphericity` Tab. 16: EffectWpp&lt;.05 time0.8688.09e-07* Mit Angaben zu dem/n Faktor(en) für den/die Sphärizität überprüft wurde der entsprechenden Teststatistik dem entsprechenden p-Wert und einer Signifikanzaussage Zum Anderen dem Ergebnis der eigentlichen Varianzanalyse: anova_time$`ANOVA` Tab. 17: EffectDFnDFdFpp&lt;.05ges time23984.330.0138*0.0125 Mit einer Angabe des Effekts, für den die Ergebnisse berichtet werden Zähler- und Nennerfreiheitsgraden den entsprechenden Quadratsummen dem F-Wert dem entsprechenden p-Wert und einer Signifikanzaussage einem Schätzer für die Effektstärke \\(\\eta^2\\) Und zu guter Letzt mit dem Ergebnis der Varianzanalyse nach Greenhouse-Geisser und Hyunh-Feldt-Korrektur anova_time$`Sphericity Corrections` Tab. 18: EffectGGep[GG]p[GG]&lt;.05HFep[HF]p[HF]&lt;.05 time0.8830.0176*0.8910.0173* Mit Einer Angabe des Effekts, für den die Ergebnisse berichtet werden dem \\(\\epsilon\\) nach Greenhouse-Geisser dem entsprechenden p-Wert und einer Signifikanzaussage dem \\(\\epsilon\\) nach Hyunh-Feldt dem entsprechenden p-Wert und einer Signifikanzaussage Aufgabe ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 2 time 2 398 4.329244 0.01380361 * 0.01245873 ## ## $`Mauchly&#39;s Test for Sphericity` ## Effect W p p&lt;.05 ## 2 time 0.8678891 8.090194e-07 * ## ## $`Sphericity Corrections` ## Effect GGe p[GG] p[GG]&lt;.05 HFe ## 2 time 0.8833057 0.01757486 * 0.8906258 ## p[HF] p[HF]&lt;.05 ## 2 0.0173103 * Wie interpretieren wir das Ergebnis? Es gibt keinen Unterschied. Da Sphärizität nicht gegeben ist, ist das Ergebnis nicht zu interpretieren. Mit Berücksichtigung des mit Sphärizitäts-Korrektur nach Huynh-Feldt signifikant gewordenen Tests nehmen wir unsere \\(\\text{H}_1\\) an, also dass sich die Skillwerte zu den Testzeitpunkten unterscheiden, es also einen Effekt der Treatments gab. Da wir die Normalverteiltheit der Variablen nicht überprüft haben, können wir keine Aussage machen. Antwort ist falsch, da Interpretation der \\(H_0\\) ist falsch, zwar ist die Voraussetzung der Sphärizität nicht gegeben, da wir aber dafür korrigieren ist dieser Umstand zu vernachlässigen. kann man so sagen Stimmt nicht ganz, aus Robustheits-Gründen wird das Ergebnis in der Regel schon valide sein. Varianzanalyse - zwei unabhängige Faktoren Zwei unabhängige Faktoren Das Modell der zweifaktoriellen Varianzanalyse erlaubt es, drei Effekte zu testen: den Haupteffekt der ersten sowie der zweiten UV und den Interaktionseffekt. Statistische \\(H_0\\)en: \\(\\begin{array}{l l c l c l c l} \\text{H}_{0A}: &amp; \\mu_{1\\cdot} &amp; = &amp; \\mu_{2\\cdot} &amp; = &amp; \\cdots &amp; = &amp; \\mu_{J\\cdot} \\\\ \\text{H}_{0B}: &amp; \\mu_{\\cdot 1} &amp; = &amp; \\mu_{\\cdot 2} &amp; = &amp; \\cdots &amp; = &amp; \\mu_{\\cdot K} \\\\ \\text{H}_{0I}: &amp; \\forall {\\mu}_{jk} &amp; \\text{ gilt}: &amp;{\\mu}_{jk} &amp; = &amp; {\\mu}_{j\\cdot}&amp; + {\\mu}_{\\cdot k} -&amp; \\mu \\end{array}\\) Die \\(H_1\\) ist dann entsprechend immer “nicht \\(H_0\\)”. Die Auswertung funktioniert dann ganz ähnlich zum einfaktoriellen Fall, wir müssen nur den zusätzlichen between-Faktor angeben. Dabei wird vom ez-Paket die folgende etwas ungewöhnliche Syntax zur Verkettung der Faktoren eingeführt: ezANOVA(data = &lt;data&gt;, dv = &lt;AV&gt;, wid = &lt;VP-Code&gt;, between = .(&lt;UV1&gt;,&lt;UV2&gt;)) Das .() ist eine extra für diesen Zweck eingeführte Liste Wenn man sich an die ungewöhnliche Notation gewöhnt hat, ist die Anwendung aber ganz leicht: library(ez) anova_groupXsex &lt;- df_wide %&gt;% ezANOVA(dv = post_skill, wid = vp_nr, between = .(group,sex)) Der Output ist dann auch nicht weiter überraschend, wir bekommen wieder eine Ergebnistabelle für die ANOVA und eine für den Levene-Test. Die Tabelle für die ANOVA ist jetzt aber natürlich etwas länger1. anova_groupXsex$ANOVA Tab. 19: EffectDFnDFdFpp&lt;.05ges group319266.5&nbsp;&nbsp;1.6e-29*0.509&nbsp; sex11921.59&nbsp;0.209&nbsp;&nbsp;0.0082 group:sex31920.6590.578&nbsp;&nbsp;0.0102 Aufgabe anova_groupXsex ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ## 1 group 3 192 66.471321 1.602553e-29 * ## 2 sex 1 192 1.586832 2.093085e-01 ## 3 group:sex 3 192 0.658699 5.784263e-01 ## ges ## 1 0.509470743 ## 2 0.008197002 ## 3 0.010187322 ## ## $`Levene&#39;s Test for Homogeneity of Variance` ## DFn DFd SSn SSd F p p&lt;.05 ## 1 7 192 9.348968 317.8641 0.8067247 0.5826385 Wie interpretieren wir das Ergebnis? Da der Levene-Test nicht signifikant geworden ist können wir keine Varianzhomogenität annehmen. Deswegen können wir die Ergebnisse nicht interpretieren. Nicht alle Effekte sind signifikant geworden, deswegen können wir nichts interpretieren. Da der Test des Haupteffekts der Gruppe signifikant geworden ist, entscheiden wir uns dazu anzunehmen, dass die unterschiedlichen Interventionen zu unterschiedlichen Skillwerten führen. Da der Interaktionseffekt nicht signifikant geworden ist, nehmen wir an, dass der Erwartungswertverlauf parallel ist. Antwort Blödsinn, genau verkehrt herum Stimmt auch nicht. Dahingegen kann man sich merken, dass der Satz “Der Interaktionseffekt ist signifikant geworden, deswegen sind die Haupteffekte schwer zu interpretieren” richtig ist. Kann man so sagen Interpretation der \\(H_0\\) Typen von Quadratsummen Voraussetzungen Die zweifaktorielle Varianzanalyse hat (ähnlich wie die einfaktorielle) die Voraussetzungen, dass die Zellen Normalverteilt mit gleichen Varianzen sind. Um das zu überprüfen, können wir einfach die uns schon bekannten Verfahren anwenden. Für die Normalverteiltheit also entweder qq-Plots: df_wide %&gt;% ggplot(aes(sample = post_skill)) + geom_qq() + geom_qq_line()+ facet_grid(sex ~ group) Oder zellenweise inferenzstatistische Tests, zum Beispiel Shapiro-Wilk: df_wide %&gt;% group_by(group, sex) %&gt;% summarise(W = shapiro.test(post_skill)$statistic, p = shapiro.test(post_skill)$p.value, &#39;sign. auf 20%-Niveau&#39; = ifelse(p &lt; .2, &#39;*&#39;, &#39;&#39;)) ## # A tibble: 8 × 5 ## # Groups: group [4] ## group sex W p ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Control f 0.945 0.246 ## 2 Control m 0.982 0.902 ## 3 Training f 0.810 0.0000782 ## 4 Training m 0.923 0.130 ## 5 Training2 f 0.978 0.836 ## 6 Training2 m 0.931 0.105 ## 7 Wartegruppe f 0.793 0.000508 ## 8 Wartegruppe m 0.913 0.0204 ## `sign. auf 20%-Niveau` ## &lt;chr&gt; ## 1 &quot;&quot; ## 2 &quot;&quot; ## 3 &quot;*&quot; ## 4 &quot;*&quot; ## 5 &quot;&quot; ## 6 &quot;*&quot; ## 7 &quot;*&quot; ## 8 &quot;*&quot; Für die Varianzhomogenität können wir einfach das von ezANOVA zurückgegebenen Levene-Test-Ergebnis nutzen: anova_groupXsex$`Levene&#39;s Test for Homogeneity of Variance` Tab. 20: DFnDFdSSnSSdFpp&lt;.05 71929.353180.8070.583 Varianzanalyse - zwei abhängige Faktoren Zwei abhängige Faktoren Statistische \\(H_0\\)en: \\(\\begin{array}{l l c l c l c l} \\text{H}_{0A}: &amp; \\mu_{1\\cdot} &amp; = &amp; \\mu_{2\\cdot} &amp; = &amp; \\cdots &amp; = &amp; \\mu_{J\\cdot} \\\\ \\text{H}_{0B}: &amp; \\mu_{\\cdot 1} &amp; = &amp; \\mu_{\\cdot 2} &amp; = &amp; \\cdots &amp; = &amp; \\mu_{\\cdot K} \\\\ \\text{H}_{0I}: &amp; \\forall {\\mu}_{jk} &amp; \\text{ gilt}: &amp;{\\mu}_{jk} &amp; = &amp; {\\mu}_{j\\cdot}&amp; + {\\mu}_{\\cdot k} -&amp; \\mu \\end{array}\\) Für die Durchführung der Zweifaktoriellen Varianzanalyse mit abhängigen Gruppen mit ezANOVA müssen natürlich wieder die zu benutzenden Faktoren im long-Format vorliegen. Die Erweiterung vom einfaktoriellen Fall funktioniert dann wieder äquivalent zum unabhängigen Fall. ezANOVA(data = &lt;data&gt;, dv = &lt;AV&gt;, wid = &lt;VP-Code&gt;, within = .(&lt;UV1&gt;,&lt;UV2&gt;)) Für diese Auswertung passt unser bisheriger Datensatz nicht so ganz. Wir benutzen deswegen kurz ein anderes Beispiel: In einem (fiktiven) Versuch zum Modell-Lernen wurde 10 Kindern an vier Terminen jeweils ein Video vorgespielt. Die Videos waren Darstellungen des folgenden Designs: Tab. 21: Aggressives ModellNicht aggressives Modell Kind als ModellAV: Fremdurteil AggressionenAV: Fremdurteil Aggressionen Jugendliche:r als ModellAV: Fremdurteil AggressionenAV: Fremdurteil Aggressionen Erwachsene:r als ModellAV: Fremdurteil AggressionenAV: Fremdurteil Aggressionen Dabei wurde die AV von einem Beobachter erhoben, der auf einem Fragebogen ein (der Einfachheit halber) normalverteiltes Urteil über das aggressive Verhalten des Kindes nach dem Versuch angegeben hat. Der zugehörige Datensatz sieht wie folgt aus: aggr_df Tab. 22: Kind_IDaggrMod_kind_aggrnAggrMod_kind_aggraggrMod_erw_aggrnAggrMod_erw_aggraggrMod_jug_aggrnAggrMod_jug_aggr 113.48.61&nbsp;9.088.37&nbsp;11.4&nbsp;5.32 211.410.6&nbsp;&nbsp;4.669.11&nbsp;10.3&nbsp;4.22 312.43.22&nbsp;9.4810.1&nbsp;&nbsp;12.5&nbsp;6.58 412.65.44&nbsp;13.6&nbsp;5.17&nbsp;9.555.64 512.45.73&nbsp;15.7&nbsp;8.51&nbsp;8.265.09 611.97.27&nbsp;8.711.85&nbsp;11.9&nbsp;5.28 713.55.43&nbsp;9.234.65&nbsp;9.385.68 811.90.6874.714.45&nbsp;13.9&nbsp;5.09 914&nbsp;&nbsp;1.12&nbsp;11.4&nbsp;-0.24310.1&nbsp;2.01 1011.98.64&nbsp;8.087.11&nbsp;12.3&nbsp;5.28 anova_behavXmodel &lt;- aggr_df %&gt;% pivot_longer(cols = -1, names_to = c(&#39;model_behav&#39;, &#39;model&#39;, &#39;.value&#39;), names_pattern = &#39;(.+)_(.+)_(.+)&#39;) %&gt;% ezANOVA(dv = aggr, wid = Kind_ID, within = .(model, model_behav)) Das Ergebnis beteht dann wie im einfaktoriellen Fall wieder aus drei Tabellen, einmal dem Ergebnis eines Mauchly-Tests für den ersten Faktor und die Interaktion: anova_behavXmodel$`Mauchly&#39;s Test for Sphericity` Tab. 23: EffectWpp&lt;.05 model0.8950.642 model:model_behav0.7460.309 Aufgabe Warum taucht kein Test für die Sphärizität des Modellverhaltens auf? Da das Modellverhalten ein unabhängiger Faktor ist. Bestimmt weil der Test des zweiten Faktors irgendwie in den anderen beiden Tests implizit ist. Da Zirkularität bei einer 2x2-Kovarianzmatrix immer gegeben ist. Weil R einen Fehler gemacht hat. Antwort ist Unsinn die Voraussetzung gilt für alle drei Variablen, wir könnten auch nicht alle Tests durchführen wollen Stimmt ist bei einem gut etablierten und getesteten Paket eher unwahrscheinlich, aber kann natürlich sein Als zweites dem Ergebnis der Varianzanalyse, natürlich wieder mit drei Zeilen für Haupteffekte und Interaktion: anova_behavXmodel$ANOVA Tab. 24: EffectDFnDFdFpp&lt;.05ges model2181.870.183&nbsp;0.0602 model_behav1944.7&nbsp;9e-05&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*0.56&nbsp;&nbsp; model:model_behav2182.720.09270.0769 Und zu guter Letzt dem Ergebnis der Varianzanalyse nach den bekannten Korrekturen: anova_behavXmodel$`Sphericity Corrections` Tab. 25: EffectGGep[GG]p[GG]&lt;.05HFep[HF]p[HF]&lt;.05 model0.9050.1871.12&nbsp;0.183&nbsp; model:model_behav0.7970.1080.9420.0968 Voraussetzungen Die abhängige zweifaktorielle Varianzanalyse hat (ähnlich wie die einfaktorielle) die Voraussetzungen, dass die Zellen Normalverteilt sind und die Kovarianzmatrix Spherizität aufweist (die Varianzen der Differenzen sind gleich). Um das zu überprüfen, können wir auch hier einfach die uns schon bekannten Verfahren anwenden. Für die Normalverteiltheit also entweder qq-Plots: aggr_df %&gt;% pivot_longer(cols = -1, names_to = c(&#39;model_behav&#39;, &#39;model&#39;, &#39;.value&#39;), names_pattern = &#39;(.+)_(.+)_(.+)&#39;) %&gt;% ggplot(aes(sample = aggr)) + geom_qq() + geom_qq_line()+ facet_grid(model ~ model_behav) Oder zellenweise inferenzstatistische Tests, zum Beispiel Shapiro-Wilk: aggr_df %&gt;% pivot_longer(cols = -1, names_to = c(&#39;model_behav&#39;, &#39;model&#39;, &#39;.value&#39;), names_pattern = &#39;(.+)_(.+)_(.+)&#39;) %&gt;% group_by(model_behav, model) %&gt;% summarise(W = shapiro.test(aggr)$statistic, p = shapiro.test(aggr)$p.value, &#39;sign. auf 20%-Niveau&#39; = ifelse(p &lt; .2, &#39;*&#39;, &#39;&#39;)) ## # A tibble: 6 × 5 ## # Groups: model_behav [2] ## model_behav model W p ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aggrMod erw 0.938 0.528 ## 2 aggrMod jug 0.976 0.938 ## 3 aggrMod kind 0.929 0.435 ## 4 nAggrMod erw 0.940 0.556 ## 5 nAggrMod jug 0.800 0.0144 ## 6 nAggrMod kind 0.951 0.675 ## `sign. auf 20%-Niveau` ## &lt;chr&gt; ## 1 &quot;&quot; ## 2 &quot;&quot; ## 3 &quot;&quot; ## 4 &quot;&quot; ## 5 &quot;*&quot; ## 6 &quot;&quot; Für die Sphärizität können wir einfach das von ezANOVA zurückgegebenen Mauchly-Test-Ergebnis nutzen: anova_behavXmodel$`Mauchly&#39;s Test for Sphericity` Tab. 26: EffectWpp&lt;.05 model0.8950.642 model:model_behav0.7460.309 Varianzanalyse - unabhängige und abhängige Faktoren Zweifaktorielle Varianzanalyse mit split-plot-Design Ein Split-Plot-Design liegt im zweifaktoriellen Fall vor, wenn within- und between-Faktoren kombiniert werden. Für die Durchführung der Zweifaktoriellen Varianzanalyse mit split-plot-Design über ezANOVA müssen natürlich genau wie in den vorangegangenen Fällen die zu benutzenden Faktoren im long-Format vorliegen. Wir wollen die Haupteffekte und die Interaktion vom Testzeitpunkt und dem Treatment untersuchen. Dafür müssen wir natürlich wieder zuerst den Datensatz pivotieren um dann entsprechend das within und das between-Argument setzen. anova_timeXgroup &lt;- df_wide %&gt;% pivot_longer(cols = contains(&#39;skill&#39;), names_to = &#39;time&#39;, values_to = &#39;skill&#39;) %&gt;% ezANOVA(dv = skill, wid = vp_nr, between = group, within = time) Die Ergebnisse sind dann wie im abhängigen Fall in die drei Tabellen für den Mauchly-Test anova_timeXgroup$`Mauchly&#39;s Test for Sphericity` Tab. 27: EffectWpp&lt;.05 time0.950.00707* group:time0.950.00707* Die Ergebnisse der ANOVA: anova_timeXgroup$ANOVA Tab. 28: EffectDFnDFdFpp&lt;.05ges group319673.2&nbsp;8.36e-32*0.265&nbsp; time23925.920.00293&nbsp;*0.0201 group:time639225.4&nbsp;1.79e-25*0.209&nbsp; Und die Sphärizitäts-Korrektur aufgeteilt. anova_timeXgroup$`Sphericity Corrections` Tab. 29: EffectGGep[GG]p[GG]&lt;.05HFep[HF]p[HF]&lt;.05 time0.9530.00343&nbsp;*0.9620.00333&nbsp;* group:time0.9532.19e-24*0.9621.35e-24* Tab. 30: EffectDFnDFdFpp&lt;.05ges group319673.2&nbsp;8.36e-32*0.265&nbsp; time23925.920.00293&nbsp;*0.0201 group:time639225.4&nbsp;1.79e-25*0.209&nbsp; Tab. 30: EffectGGep[GG]p[GG]&lt;.05HFep[HF]p[HF]&lt;.05 time0.9530.00343&nbsp;*0.9620.00333&nbsp;* group:time0.9532.19e-24*0.9621.35e-24* Wie interpretieren wir das? Es gibt eine signifikante Interaktion zwischen Untersuchungszeitpunkt und Experimentalgruppe. Der signifikante Haupteffekt von führt uns zu der Annahme, dass die Interventionen eine Veränderung über den Untersuchungsverlauf auslösen, der signifikante Haupteffekt von führt uns zu der Annahme, dass es einen Unterschied in der Wirkung der Treatments gibt. Da wir die Voraussetzungen nicht überprüft haben, können wir keine Aussage aus den Ergebnissen ziehen. Die Ergebnisse legen einen linearen Zusammenhang zwischen Untersuchungsverlauf, Experimentalgruppe und motorischem Skill nahe. Antwort Stimmt. Voraussetzungen Die zweifaktorielle Varianzanalyse im split-plot-Design hat als Voraussetzung eine Kombination der aus dem jeweiligen einfaktoriellen Fall bekannten. So setzen wir voraus, dass die AV über die unabhängigen Faktor-Stufen Varianzhomogenität aufweist und ihre Kovarinazmatrix zirkulär ist. Außerdem ist wieder Normalverteiltheit in den Zellen Voraussetzung. Um das zu überprüfen, können wir auch hier einfach die uns schon bekannten Verfahren anwenden. Für die Normalverteiltheit also wieder entweder qq-Plots: df_wide %&gt;% pivot_longer(cols = contains(&#39;skill&#39;), names_to = &#39;time&#39;, values_to = &#39;skill&#39;) %&gt;% ggplot(aes(sample = skill)) + geom_qq() + geom_qq_line()+ facet_grid(time ~ group) Oder zellenweise inferenzstatistische Tests, zum Beispiel Shapiro-Wilk: df_wide %&gt;% pivot_longer(cols = contains(&#39;skill&#39;), names_to = &#39;time&#39;, values_to = &#39;skill&#39;) %&gt;% group_by(time, group) %&gt;% summarise(W = shapiro.test(skill)$statistic, p = shapiro.test(skill)$p.value, &#39;sign. auf 20%-Niveau&#39; = ifelse(p &lt; .2, &#39;*&#39;, &#39;&#39;)) ## # A tibble: 12 × 5 ## # Groups: time [3] ## time group W p ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 peri_skill Control 0.953 0.0433 ## 2 peri_skill Training 0.968 0.184 ## 3 peri_skill Training2 0.982 0.653 ## 4 peri_skill Wartegruppe 0.928 0.00453 ## 5 post_skill Control 0.971 0.247 ## 6 post_skill Training 0.870 0.0000567 ## 7 post_skill Training2 0.977 0.441 ## 8 post_skill Wartegruppe 0.866 0.0000419 ## 9 pre_skill Control 0.953 0.0446 ## 10 pre_skill Training 0.943 0.0169 ## 11 pre_skill Training2 0.944 0.0187 ## 12 pre_skill Wartegruppe 0.897 0.000384 ## `sign. auf 20%-Niveau` ## &lt;chr&gt; ## 1 &quot;*&quot; ## 2 &quot;*&quot; ## 3 &quot;&quot; ## 4 &quot;*&quot; ## 5 &quot;&quot; ## 6 &quot;*&quot; ## 7 &quot;&quot; ## 8 &quot;*&quot; ## 9 &quot;*&quot; ## 10 &quot;*&quot; ## 11 &quot;*&quot; ## 12 &quot;*&quot; Für die Sphärizität können wir einfach das von ezANOVA zurückgegebenen Mauchly-Test-Ergebnis nutzen: anova_timeXgroup$`Mauchly&#39;s Test for Sphericity` Tab. 31: EffectWpp&lt;.05 time0.950.00707* group:time0.950.00707* Beliebige Linearkontraste Kontraste, ein Faktor Wie schon bei der Varianzanalyse mit einem unabhängigen Faktor erwähnt, können mit pairwise.t.test alle Zellvergleiche als \\(\\alpha\\)-korrigierte t-Tests durchgeführt werden. Als Alternative gibt es mit den beliebigen Linearkontrasten eine flexiblere Methode, um spezielle Vergleiche anzustreben. Jeder dieser Kontraste (\\(\\psi\\)) ist eine Linearkombination der Gruppenerwartungswerte \\(\\mu_j\\) mit den Koeffizienten \\(c_j\\), wobei die Koeffizienten in Summe 0 ergeben. Mit 1 \\(\\leq\\) j \\(\\leq\\) J könnte man entsprechend Kontraste wie folgt formulieren: \\[\\psi = \\sum_1^Jc_j\\cdot\\mu_j\\] Um diese theoretischen Kontraste zu testen, können wir den Dunns Test für mehrfache Vergleiche nutzen. Dieser nutzt die unter der Gültigkeit der Nullhypothese \\(H_0: \\psi = 0\\) \\(t_{N-J}\\)-verteilte Teststatistik \\(t = {{\\hat\\psi}\\over{\\sqrt{\\sum_1^J {c^2_j\\over{n_j}}MS_{Fehler}}}}\\). Dabei steht \\(N\\) für die Stichprobengröße, \\(MS_{w}\\) für die mittlere Quadratsumme aus dem Nenner der einfaktoriellen Varianzanalyse, \\(\\hat\\psi\\) für den empirischen Schätzer \\(\\sum_1^Jc_j\\cdot M_j\\) des Kontrasts und \\(n_j\\) für die Zellbesetzung. Da wir mit einer t-Verteilung testen, können wir natürlich auch gerichtete Hypothesen aufstellen. In R lässt sich dieser Test mit der glht (generalized linear hypothesis test)-Funktion aus dem multcomp-Paket durchführen. Dazu brauchen wir zuerst ein aov-Modell für die Quadratsumme, die wir entweder mit der aov-Funktion erstellen können… aov_mod &lt;- aov(post_skill ~ group, data = df_wide) …oder uns mit Hilfe des return_aov-Arguments der ezANOVA-Funktion mit ausgeben lassen können: ez_anova &lt;- df_wide %&gt;% ezANOVA(dv = post_skill, wid = vp_nr, between = group, return_aov = T) ez_anova$aov ## Call: ## aov(formula = formula(aov_formula), data = data) ## ## Terms: ## group Residuals ## Sum of Squares 738.9439 700.5886 ## Deg. of Freedom 3 196 ## ## Residual standard error: 1.890617 ## Estimated effects may be unbalanced Als nächsten Schritt müssen wir unsere Linearkontraste formulieren. In unserem Fall könnten wir auf die Idee kommen, die Trainings-Bedingungen gegen die Kontrollbedingung testen zu wollen. Die einfachste (und übersichtlichste) Möglichkeit diesen Kontrast zu formulieren ist es, die dem gewünschten Kontrast entsprechende Linearkombination als Text zu notieren: contr_hypothesis &lt;- &#39;2 * Control - (Training + Training2) &lt;= 0&#39; Man kann die Kontraste unter Anderem zwar auch als Matrix formulieren, die Text-Variante hat aber die entscheidenden Vorteile dass 1. die Ordnung der Faktorstufen nicht bekannt sein muss und 2. auf den ersten Blick erkennbar ist, welche Richtung welcher Hypothese getestet werden soll. Unsere Nullhypothese ist also: \\[H_0: 2\\mu_{Control} - (\\mu_{Training} + \\mu_{Training2}) = \\psi \\leq 0\\] Um diese Hypothese zu testen, müssen wir sie der mcp-Funktion auch aus dem glht-Paket übergeben. Dabei müssen wir darauf achten, dass wir den Namen des in der Varianzanalyse genutzten between-Faktors als Argument-Namen nutzen: library(multcomp) cont &lt;- mcp(group = contr_hypothesis) Mit diesen mcp-Objekt können wir nun endlich den Kontrast-Test rechnen: glht(ez_anova$aov, cont) ## ## General Linear Hypotheses ## ## Multiple Comparisons of Means: User-defined Contrasts ## ## ## Linear Hypotheses: ## Estimate ## 2 * Control - (Training + Training2) &lt;= 0 0.08774 Achtung: Das Modell muss ursprünglich mit einem factor als Gruppierungsfaktor gerechnet worden sein. Netterweise wandelt ezANOVA das aber auch für uns um. Das Ergebnis von glht lässt sich auch wieder mit broom darstellen: glht(ez_anova$aov, cont) %&gt;% broom::tidy() Tab. 32: termcontrastnull.valueestimatestd.errorstatisticadj.p.value group2 * Control - (Training + Training2)00.08770.6550.1340.447 Mit Setzen des test -Arguments können wir außerdem wieder auswählen, wie wir den p-Wert adjustieren, indem wir einen Aufruf der adjusted-Funktion übergeben. Bei einem Test müssen wir nicht unbedingt adjustieren, können also wie folgt vorgehen: glht(ez_anova$aov, cont) %&gt;% broom::tidy(test = adjusted(&#39;none&#39;)) Tab. 33: termcontrastnull.valueestimatestd.errorstatisticp.value group2 * Control - (Training + Training2)00.08770.6550.1340.447 Die Möglichkeit der Korrektur weißt schon darauf hin, dass wir diese Methode auch für multiple Vergleiche einsetzen können. Dafür müssen wir aus der einen Hypothese einfach einen Vektor aus Hypothesen machen: So könnten wir zusätzlich zum bisherigen Kontrast auch alle Zellen noch mit der Wartegruppe vergleichen wollen. Dafür würden wir das ganze wie folgt formulieren: contr_hypothesis &lt;- c(&#39;2 * Control - (Training + Training2) &lt;= 0&#39;, &#39;Wartegruppe - Control &lt;= 0&#39;, &#39;Wartegruppe - Training &lt;= 0&#39;, &#39;Wartegruppe - Training2 &lt;= 0&#39;) glht(ez_anova$aov,mcp(group = contr_hypothesis)) %&gt;% broom::tidy(test = adjusted(&#39;holm&#39;)) Tab. 34: termcontrastnull.valueestimatestd.errorstatisticadj.p.value group2 * Control - (Training + Training2)00.08770.6550.1340.447&nbsp;&nbsp;&nbsp; groupWartegruppe - Control03.9&nbsp;&nbsp;&nbsp;0.37810.3&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; groupWartegruppe - Training05.21&nbsp;&nbsp;0.37813.8&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; groupWartegruppe - Training202.67&nbsp;&nbsp;0.3787.06&nbsp;2.81e-11 Kontraste, mehr als ein Faktor Um Kontraste über mehr als einen between-Faktor rechnen zu können, müssen wir unser mehrfaktorielles Design in ein assoziiert einfaktorielles umwandeln. Dazu können wir die interaction-Funktion aus dem base-Umfang nutzen, die zwei Faktoren zu einem kombiniert. Als Beispiel betrachten wir nochmal die Unterschiede zwischen den Treatment-Gruppen und den Geschlechtern. Um eine assoziierte einfaktorielle Varianzanalyse über diese Faktoren zu rechnen, müssen wir einfach ein mutate mit einer interaction hinzufügen: df_wide_I &lt;- df_wide %&gt;% mutate(groupXsex = interaction(group, sex, sep = &#39;_&#39;)) assoz_anova &lt;- df_wide_I %&gt;% ezANOVA(dv = post_skill, wid = vp_nr, between = groupXsex, return_aov = T) Auf dieser Basis können wir dann einfach wie im einfaktoriellen Fall unsere Kontraste formulieren. Zum Beispiel können wir je einen Haupteffekts- und Interaktionseffektskontrast formulieren und testen. Da die Kombinationen der Stufen sehr viele sind, bietet es sich hier an, die Matrix-Schreibweise zu nutzen. Die Textschreibweise würde aber auch funktionieren. In die Matrix schreiben wir zeilenweise die Gewichte, die getestet werden sollen. Damit wir genau wissen, welches Gewicht an welche Stelle kommt, können wir uns zuerst die Stufen des Interaktionsfaktors ausgeben lassen: levels(df_wide_I$groupXsex) ## [1] &quot;Control_f&quot; &quot;Training_f&quot; &quot;Training2_f&quot; ## [4] &quot;Wartegruppe_f&quot; &quot;Control_m&quot; &quot;Training_m&quot; ## [7] &quot;Training2_m&quot; &quot;Wartegruppe_m&quot; Nun erstellen wir eine Matrix, bei der jede Spalte der Ordnung der Levels entsprechend dem zu gewichtenden Erwartungswert einer Zelle entspricht: contrasts &lt;- matrix( c( 1,1,1,1,-1,-1,-1,-1, # Haupteffekt des Geschlechts 1,-1,-1,1,1,-1,-1,1, # Haupteffekt des Trainings 1,-1,-1,1,-1,1,1,-1 # Interaktionseffekt (Umgekehrte Wirkung bei Männern) ), nrow = 3, byrow = T # damit zeilenweise aufgefüllt wird. ) Der Übersicht halber können wir diese Kontraste auch noch für den Output benennen, außerdem müssen wir noch Spaltennamen für die mcp-Funktion hinzufügen: colnames(contrasts) &lt;- levels(df_wide_I$groupXsex) rownames(contrasts) &lt;- c(&#39;f &gt; m&#39;, &#39;T1, T2 &lt; W, C&#39;, &#39;Interaktion&#39;) contrasts ## Control_f Training_f Training2_f ## f &gt; m 1 1 1 ## T1, T2 &lt; W, C 1 -1 -1 ## Interaktion 1 -1 -1 ## Wartegruppe_f Control_m Training_m ## f &gt; m 1 -1 -1 ## T1, T2 &lt; W, C 1 1 -1 ## Interaktion 1 -1 1 ## Training2_m Wartegruppe_m ## f &gt; m -1 -1 ## T1, T2 &lt; W, C -1 1 ## Interaktion 1 -1 Und diese Matrix können wir dann wie gewohnt für die Kontrasttests nutzen. Dabei können wir hier jetzt (da wir in der Kontrastmatrix noch keine Richtung vorgegeben haben) wie gewohnt die Testrichtung mit alternative festlegen: glht(assoz_anova$aov, mcp(groupXsex = contrasts), alternative = &#39;greater&#39;) %&gt;% broom::tidy() Tab. 35: termcontrastnull.valueestimatestd.errorstatisticadj.p.value groupXsexf &gt; m0-1.4&nbsp;&nbsp;1.08-1.29&nbsp;0.999&nbsp;&nbsp;&nbsp; groupXsexT1, T2 &lt; W, C07.67&nbsp;1.087.07&nbsp;3.47e-11 groupXsexInteraktion00.4521.080.4170.698&nbsp;&nbsp;&nbsp; "],["literatur.html", "Literatur", " Literatur "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
